{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to Data Science with Ghanashyam Check out github account as well as my gitlab account for more projects and actual code Machine Learning and Artificial Intelligence Regression Coursera (Andrew Ng) Linear Regression Logistic Regression Regularization Erdos Bootcamp Simple Linear Regression (SLR) Multiple Linear Regression (MLR) Interaction terms, polynomials and other transforms Overfitting, The Bias-Variance Trade-off and Multicollinearity Regularization with LASSO and Ridge Regression Hands On ML, Geron Some notebooks Classification Coursera (Andrew Ng) Classification: Support Vector Machine (SVM) Erdos Bootcamp Logistic Regression: The Algorithm Logistic Regression: Build a classifier Decision Trees Random Forests Categorical Data in Tree Based algorithm Support Vector Machines (SVM) Ensamble Methods Train Test Splits Oversampling and Undersampling Hands ON ML (Geron) Unsupervised Learning Coursera (Andrew Ng) Clustering (K-means) : Unsupervised Learning Dimensionality Reduction : PCA Erdos Bootcamp ClusteringClustering Practices Dimensionality Reduction: PCA Dimensionality Reduction: Manifold Learning Dimensionality Reduction: Practices Proper Data Preprocessing Steps","title":"Home"},{"location":"#welcome-to-data-science-with-ghanashyam","text":"Check out github account as well as my gitlab account for more projects and actual code","title":"Welcome to Data Science with Ghanashyam"},{"location":"#machine-learning-and-artificial-intelligence","text":"","title":"Machine Learning and Artificial Intelligence"},{"location":"#regression","text":"Coursera (Andrew Ng) Linear Regression Logistic Regression Regularization Erdos Bootcamp Simple Linear Regression (SLR) Multiple Linear Regression (MLR) Interaction terms, polynomials and other transforms Overfitting, The Bias-Variance Trade-off and Multicollinearity Regularization with LASSO and Ridge Regression Hands On ML, Geron Some notebooks","title":"Regression"},{"location":"#classification","text":"Coursera (Andrew Ng) Classification: Support Vector Machine (SVM) Erdos Bootcamp Logistic Regression: The Algorithm Logistic Regression: Build a classifier Decision Trees Random Forests Categorical Data in Tree Based algorithm Support Vector Machines (SVM) Ensamble Methods Train Test Splits Oversampling and Undersampling Hands ON ML (Geron)","title":"Classification"},{"location":"#unsupervised-learning","text":"Coursera (Andrew Ng) Clustering (K-means) : Unsupervised Learning Dimensionality Reduction : PCA Erdos Bootcamp ClusteringClustering Practices Dimensionality Reduction: PCA Dimensionality Reduction: Manifold Learning Dimensionality Reduction: Practices Proper Data Preprocessing Steps","title":"Unsupervised Learning"},{"location":"whycode/","text":"This is whycode page? For full documentation you can visit mkdocs.org . Commands why code? Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Why Codes?"},{"location":"whycode/#this-is-whycode-page","text":"For full documentation you can visit mkdocs.org .","title":"This is whycode page?"},{"location":"whycode/#commands","text":"why code?","title":"Commands"},{"location":"whycode/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"machinelearning/compare_classification/","text":"How to Evaluate Gradient Boosting Models with XGBoost in Python Source: https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/ Prepare the data ## Install xgboost as ##!pip3 install xgboost import xgboost print ( 'xgboost version:' , xgboost . __version__) xgboost version: 0.90 import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd . read_csv( './data/pima-indians-diabetes.data.csv' , header = None ) print ( 'shape(data):' ,data . shape) shape(data): (768, 9) data . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 # the last column is the yes/no column where 0 represents NO diabetes and 1 represent YES daibetes y = data . iloc[:, - 1 ] X = data . iloc[:,: - 1 ] X . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 0 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33 , random_state = 1728 ) print ( 'X_train.shape, X_test.shape, y_train.shape, y_test.shape' ) print (X_train . shape, X_test . shape, y_train . shape, y_test . shape) X_train.shape, X_test.shape, y_train.shape, y_test.shape (514, 8) (254, 8) (514,) (254,) Clasiffication using different Classifiers XGBOOST XGBoost article on towardsdatascience from xgboost import XGBClassifier xgb = XGBClassifier() xgb . fit(X_train, y_train) # 'eval_set' is another arguement for the validation data but not used here. y_pred = xgb . predict(X_test) preds = [ round (yp) for yp in y_pred] #accuracy can also be measured as #score = xgb.score(X_test, y_test) from sklearn.metrics import accuracy_score acc = accuracy_score(y_test, preds) print ( \"accuracy= %.2f \" % (acc * 100 )) accuracy=77.56 XGBOOST with With k-fold cross validation from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score kfold = KFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.38 from sklearn.model_selection import StratifiedKFold kfold = StratifiedKFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.66 If in doubt, use 10-fold cross validation for regression problems and stratified 10-fold cross validation on classification problems. Support Vector Machine from sklearn.svm import SVC clf_ = SVC(gamma = 2 , C = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"SVM Classifier, score : %.2f \" % (score * 100 )) SVM Classifier, score : 66.54 K-Nearest Neighbour from sklearn.neighbors import KNeighborsClassifier clf_ = KNeighborsClassifier( 3 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"K nearest neighbour Classifier, score : %.2f \" % (score * 100 )) K nearest neighbour Classifier, score : 71.65 Random Forrest from sklearn.ensemble import RandomForestClassifier clf_ = RandomForestClassifier(max_depth = 5 , n_estimators = 10 , max_features = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Random Forest Classifier, score : %.2f \" % (score * 100 )) Random Forest Classifier, score : 75.98 Decision Tree from sklearn.tree import DecisionTreeClassifier clf_ = DecisionTreeClassifier(max_depth = 5 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Decision Tree Classifier, score : %.2f \" % (score * 100 )) Decision Tree Classifier, score : 75.98 Adaboost I don't know much about it. from sklearn.ensemble import AdaBoostClassifier clf_ = AdaBoostClassifier() clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Adaboost Classifier, score : %.2f \" % (score * 100 )) Adaboost Classifier, score : 75.98","title":"comarision"},{"location":"machinelearning/compare_classification/#how-to-evaluate-gradient-boosting-models-with-xgboost-in-python","text":"Source: https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/","title":"How to Evaluate Gradient Boosting Models with XGBoost in Python"},{"location":"machinelearning/compare_classification/#prepare-the-data","text":"## Install xgboost as ##!pip3 install xgboost import xgboost print ( 'xgboost version:' , xgboost . __version__) xgboost version: 0.90 import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd . read_csv( './data/pima-indians-diabetes.data.csv' , header = None ) print ( 'shape(data):' ,data . shape) shape(data): (768, 9) data . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 # the last column is the yes/no column where 0 represents NO diabetes and 1 represent YES daibetes y = data . iloc[:, - 1 ] X = data . iloc[:,: - 1 ] X . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 0 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33 , random_state = 1728 ) print ( 'X_train.shape, X_test.shape, y_train.shape, y_test.shape' ) print (X_train . shape, X_test . shape, y_train . shape, y_test . shape) X_train.shape, X_test.shape, y_train.shape, y_test.shape (514, 8) (254, 8) (514,) (254,)","title":"Prepare the data"},{"location":"machinelearning/compare_classification/#clasiffication-using-different-classifiers","text":"","title":"Clasiffication using different Classifiers"},{"location":"machinelearning/compare_classification/#xgboost","text":"XGBoost article on towardsdatascience from xgboost import XGBClassifier xgb = XGBClassifier() xgb . fit(X_train, y_train) # 'eval_set' is another arguement for the validation data but not used here. y_pred = xgb . predict(X_test) preds = [ round (yp) for yp in y_pred] #accuracy can also be measured as #score = xgb.score(X_test, y_test) from sklearn.metrics import accuracy_score acc = accuracy_score(y_test, preds) print ( \"accuracy= %.2f \" % (acc * 100 )) accuracy=77.56","title":"XGBOOST"},{"location":"machinelearning/compare_classification/#xgboost-with-with-k-fold-cross-validation","text":"from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score kfold = KFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.38 from sklearn.model_selection import StratifiedKFold kfold = StratifiedKFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.66 If in doubt, use 10-fold cross validation for regression problems and stratified 10-fold cross validation on classification problems.","title":"XGBOOST with With k-fold cross validation"},{"location":"machinelearning/compare_classification/#support-vector-machine","text":"from sklearn.svm import SVC clf_ = SVC(gamma = 2 , C = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"SVM Classifier, score : %.2f \" % (score * 100 )) SVM Classifier, score : 66.54","title":"Support Vector Machine"},{"location":"machinelearning/compare_classification/#k-nearest-neighbour","text":"from sklearn.neighbors import KNeighborsClassifier clf_ = KNeighborsClassifier( 3 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"K nearest neighbour Classifier, score : %.2f \" % (score * 100 )) K nearest neighbour Classifier, score : 71.65","title":"K-Nearest Neighbour"},{"location":"machinelearning/compare_classification/#random-forrest","text":"from sklearn.ensemble import RandomForestClassifier clf_ = RandomForestClassifier(max_depth = 5 , n_estimators = 10 , max_features = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Random Forest Classifier, score : %.2f \" % (score * 100 )) Random Forest Classifier, score : 75.98","title":"Random Forrest"},{"location":"machinelearning/compare_classification/#decision-tree","text":"from sklearn.tree import DecisionTreeClassifier clf_ = DecisionTreeClassifier(max_depth = 5 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Decision Tree Classifier, score : %.2f \" % (score * 100 )) Decision Tree Classifier, score : 75.98","title":"Decision Tree"},{"location":"machinelearning/compare_classification/#adaboost","text":"I don't know much about it. from sklearn.ensemble import AdaBoostClassifier clf_ = AdaBoostClassifier() clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Adaboost Classifier, score : %.2f \" % (score * 100 )) Adaboost Classifier, score : 75.98","title":"Adaboost"},{"location":"machinelearning/file0/","text":"Welcome to MkDocs For full documentation you can visit mkdocs.org . Commands ls -la : is the command for listing files Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"file0"},{"location":"machinelearning/file0/#welcome-to-mkdocs","text":"For full documentation you can visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"machinelearning/file0/#commands","text":"ls -la : is the command for listing files","title":"Commands"},{"location":"machinelearning/file0/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"}]}