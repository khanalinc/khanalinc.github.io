{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Welcome to learning with data Check out github account as well as my gitlab account for more projects and actual code You'll find some interesting notes on the following here. Linux/Unix: Vi, Sed, Awk, Grep, Egrep Python: Numpy, Scipy, Pandas, Matplotlib Machine Learning and Artificial Intelligence Quantum Computing Algorithms","title":"Home"},{"location":"#welcome-to-learning-with-data","text":"Check out github account as well as my gitlab account for more projects and actual code You'll find some interesting notes on the following here.","title":"Welcome to learning with data"},{"location":"#linuxunix-vi-sed-awk-grep-egrep","text":"","title":"Linux/Unix: Vi, Sed, Awk, Grep, Egrep"},{"location":"#python-numpy-scipy-pandas-matplotlib","text":"","title":"Python: Numpy, Scipy, Pandas, Matplotlib"},{"location":"#machine-learning-and-artificial-intelligence","text":"","title":"Machine Learning and Artificial Intelligence"},{"location":"#quantum-computing","text":"","title":"Quantum Computing"},{"location":"#algorithms","text":"","title":"Algorithms"},{"location":"linux/command_linux/","text":"Working on Unix/Linux I have been using Linux OS for some time now, for research and my other work related stuffs. Here I am sharing my notes. Vi Editor basically has two modes: INSERT MODE and COMMAND MODE . In Insert mode, one can edit, make changes. On the other hand in command mode, we can enter commands to perform certain task, for example substitution, deletion. To switch from INSERT MODE to COMMAND MODE press Esc key. To switch from COMMAND MODE to INSERT MODE press I . Basics commands i : Insert text at the current cursor position I : Insert text at the begining of the current line. o : Add text in a new line below the current line. O : Add text in a new line above the current line. a : Append text after the position of cursor. A : Append at the end of line. Move Cursor h : Move the cursor one character to the left l : Move the cursor one character to the right k : Move the cursor up one line j : Move the cursor down one line gg : Go to the begining of file. G : Go to the end of the file. ^d : Shift one page down [^: CTRL] ^u : Shift one page up [^: CTRL] $ : Move cursor to the end of current line 0 : Move cursor to the beginning of current line w : Forward one word b : Backward one word Cut / Copy / Paste Delete x : Delete a character dd : Delete current line d$ : Delete from the position of the cursor to end of the line dw : Delete word from cursor on db : Delete word backward Copy (Yank) yy : Yank current line y$ : Yank to end of current line from cursor yw : Yank from cursor to end of current word 2yy : Yank 2 lines y : Yanks the Highlighted characters. Paste p : Paste below the cursor P : Paste above the cursor u : Undo last change U : Restore line J : Join next line down to the end of the current line Highlight characters v : activates the visual mode and one can move cursor to highlight required characters. ^v : Highlight vertically. Very useful for inserting tab in multiple lines. Exit from the Editor :wq : Write file to disk and quit the editor :q! : Quit (Without saving the changes) :q : Quit (Will print out a warning if changes have been made) :w abc : Save the current file to a new file abc more commands Find and Replace One of the widely used commands for me personally. :[range]s/{pattern}/{string}/[flags] [count] Possible Flags: [c] : Confirm each substitution [g] : Replace all occurrrences in the line. [i] : Ignore case for the pattern s : stands for substitute Some examples :s/XX/YY/g [only in current line] :%s/XX/YY/g [in the entire file] :5,12s/XX/YY/g [only in specific lines 5 and 12] :.,$s/XX/YY/g [current + all the following lines.] :s/XX/YY/g 4 [Current + following 4 lines] :.,+2s/XX/YY/g [current + next 2 lines same as above.] Explanation :s for substitution, . : current line , $ :end line, % : entire file, g : global search but locally global. only for that line. insert spaces or indentation in multiple lines highlight the line[s] and press >> or << deleting certain parts :%normal 2x [Delete the first n characters of every lines] :%s/^ / [Delete the first n characters of every lines only if they are spaces] :%s/\\s+$//e [Delete any trailing space at the end of all the lines] :%s/ :%s/^\\s\\+//e [Delete all initial spaces(or tabs) of all the lines] [^: begining of line; \\s looks for spaces(including tabs); \\+ looks for one and or more occurance; //e: first / means substitute by and nothing after it means nothing and \\e means if not found the string then suppress the error.] :%le [==:left] [Global left alignment (much more efficient)] OPEN/ CLOSE/ SPLIT files vim f1.txt [open single file] vim -o f1.txt f2.txt [open two files in horizontal] vim -O f1.txt f2.txt [open two files vertically] CTRL w [R/L/T/B arrow] [toggle between different split windows] CTRL +w w [ Toggle between different split windows] :e new_file [open new file in this window] :sp new_file [splits Horizontally and opens the new file] :vsp new_file [splits vertically and opens the new file] CTRL +w , s [split Horizontally ] CTRL +w , v [split vertically ] CTRL +w, Q [Quit the current window] g lobal command The global command is useful in many cases. general pattern: :[range]g/pattern/cmd !!Examples :g/[abcde]/d [delete lines matching pattern abcde] :!g/[abcde]/d [delete lines not matching the pattern abcde] :g/^\\s*$/d [^($) means start(end) of line and \\s* means 0 or many spaces including tab, delete all empty lines.] :g/[pat]/s/abc/def/gi [In all lines containing pattern pat, substitute abc by def.] Find a pattern in a line and delete everything after it. or delete after that pattern :1,$s/pattern.*//g . # to delete everything after (and including) that pattern :1,$s/pattern.*/pattern/g . # to delete everything after (excluding) that pattern Some other Examples Sometime you have to comment few lines. For that Usually for bash-script we put a # sign in the front of every line to comment that line. Easier way is following: go to top(bottom) of the lines press CTRL v to go to visual mode and press j or k to go down or up to whatever many lines you need to comment. Press I (capital i) and put a sign # now go to command mode by pressing Esc key and save the file :w . Should work! Playing with tab and spaces in VIM A lot of times I get old scripts and codes where a tab is used and while making changes and compiling the code I get indentation error due to tabs. Here is how to fix this. Replace tab by spaces: :retab Set length of spaces for a tab: :set tabstop=4 , :set shiftwidth=4 , :set expandtab For more info, find my .vimrc file my vimrc file Yank and Paste same thing multiple times. xnoremap p pgvy p to paste, gv to re-select what was originally selected. y to copy it again. Grep and Egrep commands -i [ignore case] -v [invert the match] -o [only the matching] -A [num] [print the [num] lines after the match] -B [num] [print the [num] lines before the match] First grep something from a file and then find the last line from that grepped file. find FeSe.scf |xargs grep FGL002 |tail -1 Finds FGL002 in file .scf and returns the last line Find only specific numbers from a line less log.ctqmc | egrep -o \"<Sz>=-?[0-9].[0-9]+\" | egrep -oe \u2019-?[0-9].[0-9]+\u2019 > sz_data Find only the numbers from the grepped entity. use regular expression. egrep is for extended grep. find FeSe.scf |xargs grep :FER |tail -1| egrep -o [0-9].[0-9]+ !Regular Expressions ^[-+]?[0-9]*\\.?[0-9]+$ ^ - start of string $ - the end of string [0-9]? - 0 or 1 sign indicator [0-9]* - 0 or more integers [0-9]+ - 1 or more integers \\. - the charcter (. is used in regex to mean \"any character\") Sed and Awk Commands p print d delete s substitute a append i insert c change y transform q quit !! Things to note $ represents the last line ^$ represents blank line. sed Syntax sed [-n] [-e] ['command'] [file] -n only prints line specified with the print command -e The next argument is a filename containing editing commands print Command p [ p rint only certain lines] Syntax: [address1[,address2]]p Examples: sed -n '5,10p' file.dat Print lines from line 5 to line 10 sed -n '5p' file.dat Print only 5th line sed -n '5,$p' file.dat Print lines from 5th to last line delete Command d [ d elete certain line] Syntax: [address1[,address2]]d !!Examples: sed '5d' file.dat [Delete line 5 while printing] sed '5,10d' file.dat [Delete lines from 5th to 10th while printing] sed '/^$d' file.dat [Delete line 5 while printing] sed '/^$/,$d' file.dat [Delete from first blank line through last line.] sed '/str/,$d' file.dat [Delete all lines after the line containing string **str**. [Also has to be single quotation mark.] Substitute Command s [ s ubstitute AA by BB ] Syntax: [address(es)]s/pattern/replacement/[flags] flags: n [nth occurrence] : g [global occurrence] Examples: sed \u2018s/ AA/ BB/\u2019 file.dat Only the first occurrence sed \u2018s/ AA/ BB/g\u2019 file.dat global (all) occurrences sed -i \u2018s/ AA/ BB/g\u2019 file.dat replace in file (all) occurrences For a pattern which occurs multiple times in a file, replace only some of them. [Remember sig.inp for ferromagnetic calculations] sed 's/AA/BB/2' Replace only 2nd occurrence sed 's/AA/BB/2g' Replace all after 2nd occurrence !!Note: while using sed in bash shell, be careful about '{text}' and \"{text}\" the later has been useful. Learn when to use one and when the other. More information can be on A great course for learning UNIX http://www.cs.nyu.edu/~mohri/unix08/ Basic Unix commands http://unixcommandstutorial.blogspot.com/2013/04/unixawkperl-sqlplsql-examples.html some tutorials on awk http://www.grymoire.com/Unix/Awk.html#uh-14 awk awk can be as useful as sed and in many cases even better. print only some columns of some output awk '{print $2}' file.txt [print 2nd column] grep 'str' file.txt |awk '{print $2}' [print 2nd column of the output] grep 'str' file.txt |awk '{print $2 $3}' [print 2nd and 3rd column] print only some rows from file awk 'NR==1{print}' file.txt [print 1st line] awk 'NR==1;END{print}' file.txt [print first and last line.] awk 'NR==1;END{print $2}' file.txt [print 2nd column of first and last line.] find pattern and print only certain column awk '/pattern/{print $1}' file [find lines with pattern and print first column] !Sed/Awk inside a bash script Insied a bash script you have to be little careful using sed and awk, especially if you are using a variable inside a sed command. For example the following command inside a bash script sed -i 's/${var1}/${var2}' filename.dat won't work . First thing is it needs double quotes ** as opposed to single quotes ' and the second thing is the / sign must be replaced by apersand symbol & . So the correct command would be sed -i **s&${var1}&${var2}&g** filename.dat Sed deleting last n lines of a file Option1: delete one line at a time for n times. very expensive for large files. for i in `seq 1 n`; do sed -i '${d}' file.txt Option2: print the file in reverse using tac and cut the first n lines and reverse the file once again using tac . tac file.txt |sed '1,n{d}' |tac > file_tmp mv file_tmp file.txt Option3: Found in the webpage sedoneliner sed -i -e :a -e '$d;N;2,4ba' -e 'P;D' test_file Some other nice comands sed -i -e \"1,${n0}d\" $f # works to delete lines from 1 to n0 sed -i -e \"${n0},${n1}d\" $f # deletes lines from n0 to n1 Git commands (basics) The most basic steps Initialize a project Initializing a new project. First go to the project in Gitlab web page and start a new project. For Existing directory cd existing_folder git init git remote add origin git@gitlab.com:khanalg44/Cif2Struct.git git add . git commit git push -u origin master To clone a new project to the terminal git clone <path of the repository> git clone git@gitlab.com:khanalg44/project_name.git To pull the files from the git git pull To check the status of the files git status To update/save a new change git add <filename> or git add . # Add the file/files that you want to push to repo. git commit -m 'some message # Commit the changes with some message . git push -u origin master # Push the file to the main repo Questions/Error Key Error If you have public key error Follow the steps: 1st generate new key ssh-keygen -t rsa -C \"khanalg44@gmail.com\" -b 4096 Or simply create the ssh-public-key by ssh-keygen -t rsa copy the key from id_rsa.pub to the settings in gitlab.com-> profile -> settings -> SSH keys -> paste your key here click add key Now reload your git webpage. And now git pull, git push should work. How to add branches in a git repository? here is a nice webpage for this In short, here are the steps, git branch <branchname> git status git checkout <branchname> #To go into the new branch git push origin <branchname> Reference http://rogerdudler.github.io/git-guide/ https://bitbucket.org/dasarpmar/tutorial/overview https://www.atlassian.com/git/tutorials/setting-up-a-repository/git-init","title":"Working on Unix/Linux"},{"location":"linux/command_linux/#working-on-unixlinux","text":"I have been using Linux OS for some time now, for research and my other work related stuffs. Here I am sharing my notes. Vi Editor basically has two modes: INSERT MODE and COMMAND MODE . In Insert mode, one can edit, make changes. On the other hand in command mode, we can enter commands to perform certain task, for example substitution, deletion. To switch from INSERT MODE to COMMAND MODE press Esc key. To switch from COMMAND MODE to INSERT MODE press I .","title":"Working on Unix/Linux"},{"location":"linux/command_linux/#basics-commands","text":"i : Insert text at the current cursor position I : Insert text at the begining of the current line. o : Add text in a new line below the current line. O : Add text in a new line above the current line. a : Append text after the position of cursor. A : Append at the end of line. Move Cursor h : Move the cursor one character to the left l : Move the cursor one character to the right k : Move the cursor up one line j : Move the cursor down one line gg : Go to the begining of file. G : Go to the end of the file. ^d : Shift one page down [^: CTRL] ^u : Shift one page up [^: CTRL] $ : Move cursor to the end of current line 0 : Move cursor to the beginning of current line w : Forward one word b : Backward one word Cut / Copy / Paste Delete x : Delete a character dd : Delete current line d$ : Delete from the position of the cursor to end of the line dw : Delete word from cursor on db : Delete word backward Copy (Yank) yy : Yank current line y$ : Yank to end of current line from cursor yw : Yank from cursor to end of current word 2yy : Yank 2 lines y : Yanks the Highlighted characters. Paste p : Paste below the cursor P : Paste above the cursor u : Undo last change U : Restore line J : Join next line down to the end of the current line Highlight characters v : activates the visual mode and one can move cursor to highlight required characters. ^v : Highlight vertically. Very useful for inserting tab in multiple lines. Exit from the Editor :wq : Write file to disk and quit the editor :q! : Quit (Without saving the changes) :q : Quit (Will print out a warning if changes have been made) :w abc : Save the current file to a new file abc","title":"Basics commands"},{"location":"linux/command_linux/#more-commands","text":"Find and Replace One of the widely used commands for me personally. :[range]s/{pattern}/{string}/[flags] [count] Possible Flags: [c] : Confirm each substitution [g] : Replace all occurrrences in the line. [i] : Ignore case for the pattern s : stands for substitute Some examples :s/XX/YY/g [only in current line] :%s/XX/YY/g [in the entire file] :5,12s/XX/YY/g [only in specific lines 5 and 12] :.,$s/XX/YY/g [current + all the following lines.] :s/XX/YY/g 4 [Current + following 4 lines] :.,+2s/XX/YY/g [current + next 2 lines same as above.] Explanation :s for substitution, . : current line , $ :end line, % : entire file, g : global search but locally global. only for that line. insert spaces or indentation in multiple lines highlight the line[s] and press >> or << deleting certain parts :%normal 2x [Delete the first n characters of every lines] :%s/^ / [Delete the first n characters of every lines only if they are spaces] :%s/\\s+$//e [Delete any trailing space at the end of all the lines] :%s/ :%s/^\\s\\+//e [Delete all initial spaces(or tabs) of all the lines] [^: begining of line; \\s looks for spaces(including tabs); \\+ looks for one and or more occurance; //e: first / means substitute by and nothing after it means nothing and \\e means if not found the string then suppress the error.] :%le [==:left] [Global left alignment (much more efficient)] OPEN/ CLOSE/ SPLIT files vim f1.txt [open single file] vim -o f1.txt f2.txt [open two files in horizontal] vim -O f1.txt f2.txt [open two files vertically] CTRL w [R/L/T/B arrow] [toggle between different split windows] CTRL +w w [ Toggle between different split windows] :e new_file [open new file in this window] :sp new_file [splits Horizontally and opens the new file] :vsp new_file [splits vertically and opens the new file] CTRL +w , s [split Horizontally ] CTRL +w , v [split vertically ] CTRL +w, Q [Quit the current window] g lobal command The global command is useful in many cases. general pattern: :[range]g/pattern/cmd !!Examples :g/[abcde]/d [delete lines matching pattern abcde] :!g/[abcde]/d [delete lines not matching the pattern abcde] :g/^\\s*$/d [^($) means start(end) of line and \\s* means 0 or many spaces including tab, delete all empty lines.] :g/[pat]/s/abc/def/gi [In all lines containing pattern pat, substitute abc by def.] Find a pattern in a line and delete everything after it. or delete after that pattern :1,$s/pattern.*//g . # to delete everything after (and including) that pattern :1,$s/pattern.*/pattern/g . # to delete everything after (excluding) that pattern Some other Examples Sometime you have to comment few lines. For that Usually for bash-script we put a # sign in the front of every line to comment that line. Easier way is following: go to top(bottom) of the lines press CTRL v to go to visual mode and press j or k to go down or up to whatever many lines you need to comment. Press I (capital i) and put a sign # now go to command mode by pressing Esc key and save the file :w . Should work!","title":"more commands"},{"location":"linux/command_linux/#playing-with-tab-and-spaces-in-vim","text":"A lot of times I get old scripts and codes where a tab is used and while making changes and compiling the code I get indentation error due to tabs. Here is how to fix this. Replace tab by spaces: :retab Set length of spaces for a tab: :set tabstop=4 , :set shiftwidth=4 , :set expandtab For more info, find my .vimrc file my vimrc file","title":"Playing with tab and spaces in VIM"},{"location":"linux/command_linux/#yank-and-paste-same-thing-multiple-times","text":"xnoremap p pgvy p to paste, gv to re-select what was originally selected. y to copy it again.","title":"Yank and Paste same thing multiple times."},{"location":"linux/command_linux/#grep-and-egrep","text":"","title":"Grep and Egrep"},{"location":"linux/command_linux/#commands","text":"-i [ignore case] -v [invert the match] -o [only the matching] -A [num] [print the [num] lines after the match] -B [num] [print the [num] lines before the match] First grep something from a file and then find the last line from that grepped file. find FeSe.scf |xargs grep FGL002 |tail -1 Finds FGL002 in file .scf and returns the last line Find only specific numbers from a line less log.ctqmc | egrep -o \"<Sz>=-?[0-9].[0-9]+\" | egrep -oe \u2019-?[0-9].[0-9]+\u2019 > sz_data Find only the numbers from the grepped entity. use regular expression. egrep is for extended grep. find FeSe.scf |xargs grep :FER |tail -1| egrep -o [0-9].[0-9]+ !Regular Expressions ^[-+]?[0-9]*\\.?[0-9]+$ ^ - start of string $ - the end of string [0-9]? - 0 or 1 sign indicator [0-9]* - 0 or more integers [0-9]+ - 1 or more integers \\. - the charcter (. is used in regex to mean \"any character\")","title":"commands"},{"location":"linux/command_linux/#sed-and-awk","text":"","title":"Sed and Awk"},{"location":"linux/command_linux/#commands_1","text":"p print d delete s substitute a append i insert c change y transform q quit !! Things to note $ represents the last line ^$ represents blank line.","title":"Commands"},{"location":"linux/command_linux/#sed-syntax","text":"sed [-n] [-e] ['command'] [file] -n only prints line specified with the print command -e The next argument is a filename containing editing commands","title":"sed Syntax"},{"location":"linux/command_linux/#print","text":"Command p [ p rint only certain lines] Syntax: [address1[,address2]]p Examples: sed -n '5,10p' file.dat Print lines from line 5 to line 10 sed -n '5p' file.dat Print only 5th line sed -n '5,$p' file.dat Print lines from 5th to last line","title":"print"},{"location":"linux/command_linux/#delete","text":"Command d [ d elete certain line] Syntax: [address1[,address2]]d !!Examples: sed '5d' file.dat [Delete line 5 while printing] sed '5,10d' file.dat [Delete lines from 5th to 10th while printing] sed '/^$d' file.dat [Delete line 5 while printing] sed '/^$/,$d' file.dat [Delete from first blank line through last line.] sed '/str/,$d' file.dat [Delete all lines after the line containing string **str**. [Also has to be single quotation mark.]","title":"delete"},{"location":"linux/command_linux/#substitute","text":"Command s [ s ubstitute AA by BB ] Syntax: [address(es)]s/pattern/replacement/[flags] flags: n [nth occurrence] : g [global occurrence]","title":"Substitute"},{"location":"linux/command_linux/#examples","text":"sed \u2018s/ AA/ BB/\u2019 file.dat Only the first occurrence sed \u2018s/ AA/ BB/g\u2019 file.dat global (all) occurrences sed -i \u2018s/ AA/ BB/g\u2019 file.dat replace in file (all) occurrences For a pattern which occurs multiple times in a file, replace only some of them. [Remember sig.inp for ferromagnetic calculations] sed 's/AA/BB/2' Replace only 2nd occurrence sed 's/AA/BB/2g' Replace all after 2nd occurrence !!Note: while using sed in bash shell, be careful about '{text}' and \"{text}\" the later has been useful. Learn when to use one and when the other. More information can be on A great course for learning UNIX http://www.cs.nyu.edu/~mohri/unix08/ Basic Unix commands http://unixcommandstutorial.blogspot.com/2013/04/unixawkperl-sqlplsql-examples.html some tutorials on awk http://www.grymoire.com/Unix/Awk.html#uh-14","title":"Examples:"},{"location":"linux/command_linux/#awk","text":"awk can be as useful as sed and in many cases even better. print only some columns of some output awk '{print $2}' file.txt [print 2nd column] grep 'str' file.txt |awk '{print $2}' [print 2nd column of the output] grep 'str' file.txt |awk '{print $2 $3}' [print 2nd and 3rd column] print only some rows from file awk 'NR==1{print}' file.txt [print 1st line] awk 'NR==1;END{print}' file.txt [print first and last line.] awk 'NR==1;END{print $2}' file.txt [print 2nd column of first and last line.] find pattern and print only certain column awk '/pattern/{print $1}' file [find lines with pattern and print first column] !Sed/Awk inside a bash script Insied a bash script you have to be little careful using sed and awk, especially if you are using a variable inside a sed command. For example the following command inside a bash script sed -i 's/${var1}/${var2}' filename.dat won't work . First thing is it needs double quotes ** as opposed to single quotes ' and the second thing is the / sign must be replaced by apersand symbol & . So the correct command would be sed -i **s&${var1}&${var2}&g** filename.dat","title":"awk"},{"location":"linux/command_linux/#sed-deleting-last-n-lines-of-a-file","text":"Option1: delete one line at a time for n times. very expensive for large files. for i in `seq 1 n`; do sed -i '${d}' file.txt Option2: print the file in reverse using tac and cut the first n lines and reverse the file once again using tac . tac file.txt |sed '1,n{d}' |tac > file_tmp mv file_tmp file.txt Option3: Found in the webpage sedoneliner sed -i -e :a -e '$d;N;2,4ba' -e 'P;D' test_file Some other nice comands sed -i -e \"1,${n0}d\" $f # works to delete lines from 1 to n0 sed -i -e \"${n0},${n1}d\" $f # deletes lines from n0 to n1","title":"Sed deleting last n lines of a file"},{"location":"linux/command_linux/#git-commands-basics","text":"","title":"Git commands (basics)"},{"location":"linux/command_linux/#the-most-basic-steps","text":"Initialize a project Initializing a new project. First go to the project in Gitlab web page and start a new project. For Existing directory cd existing_folder git init git remote add origin git@gitlab.com:khanalg44/Cif2Struct.git git add . git commit git push -u origin master To clone a new project to the terminal git clone <path of the repository> git clone git@gitlab.com:khanalg44/project_name.git To pull the files from the git git pull To check the status of the files git status To update/save a new change git add <filename> or git add . # Add the file/files that you want to push to repo. git commit -m 'some message # Commit the changes with some message . git push -u origin master # Push the file to the main repo","title":"The most basic steps"},{"location":"linux/command_linux/#questionserror","text":"","title":"Questions/Error"},{"location":"linux/command_linux/#key-error","text":"If you have public key error Follow the steps: 1st generate new key ssh-keygen -t rsa -C \"khanalg44@gmail.com\" -b 4096 Or simply create the ssh-public-key by ssh-keygen -t rsa copy the key from id_rsa.pub to the settings in gitlab.com-> profile -> settings -> SSH keys -> paste your key here click add key Now reload your git webpage. And now git pull, git push should work.","title":"Key Error"},{"location":"linux/command_linux/#how-to-add-branches-in-a-git-repository","text":"here is a nice webpage for this In short, here are the steps, git branch <branchname> git status git checkout <branchname> #To go into the new branch git push origin <branchname>","title":"How to add branches in a git repository?"},{"location":"linux/command_linux/#reference","text":"http://rogerdudler.github.io/git-guide/ https://bitbucket.org/dasarpmar/tutorial/overview https://www.atlassian.com/git/tutorials/setting-up-a-repository/git-init","title":"Reference"},{"location":"linux/git/","text":"Git commands (basics) The most basic steps Initialize a project Initializing a new project. First go to the project in Gitlab web page and start a new project. For Existing directory cd existing_folder git init git remote add origin git@gitlab.com:khanalg44/Cif2Struct.git git add . git commit git push -u origin master To clone a new project to the terminal git clone <path of the repository> git clone git@gitlab.com:khanalg44/project_name.git To pull the files from the git git pull To check the status of the files git status To update/save a new change git add <filename> or git add . # Add the file/files that you want to push to repo. git commit -m 'some message # Commit the changes with some message . git push -u origin master # Push the file to the main repo Questions/Error Key Error If you have public key error Follow the steps: 1st generate new key ssh-keygen -t rsa -C \"khanalg44@gmail.com\" -b 4096 Or simply create the ssh-public-key by ssh-keygen -t rsa copy the key from id_rsa.pub to the settings in gitlab.com-> profile -> settings -> SSH keys -> paste your key here click add key Now reload your git webpage. And now git pull, git push should work. How to add branches in a git repository? here is a nice webpage for this In short, here are the steps, git branch <branchname> git status git checkout <branchname> #To go into the new branch git push origin <branchname> Reference http://rogerdudler.github.io/git-guide/ https://bitbucket.org/dasarpmar/tutorial/overview https://www.atlassian.com/git/tutorials/setting-up-a-repository/git-init","title":"Git commands (basics)"},{"location":"linux/git/#git-commands-basics","text":"","title":"Git commands (basics)"},{"location":"linux/git/#the-most-basic-steps","text":"Initialize a project Initializing a new project. First go to the project in Gitlab web page and start a new project. For Existing directory cd existing_folder git init git remote add origin git@gitlab.com:khanalg44/Cif2Struct.git git add . git commit git push -u origin master To clone a new project to the terminal git clone <path of the repository> git clone git@gitlab.com:khanalg44/project_name.git To pull the files from the git git pull To check the status of the files git status To update/save a new change git add <filename> or git add . # Add the file/files that you want to push to repo. git commit -m 'some message # Commit the changes with some message . git push -u origin master # Push the file to the main repo","title":"The most basic steps"},{"location":"linux/git/#questionserror","text":"","title":"Questions/Error"},{"location":"linux/git/#key-error","text":"If you have public key error Follow the steps: 1st generate new key ssh-keygen -t rsa -C \"khanalg44@gmail.com\" -b 4096 Or simply create the ssh-public-key by ssh-keygen -t rsa copy the key from id_rsa.pub to the settings in gitlab.com-> profile -> settings -> SSH keys -> paste your key here click add key Now reload your git webpage. And now git pull, git push should work.","title":"Key Error"},{"location":"linux/git/#how-to-add-branches-in-a-git-repository","text":"here is a nice webpage for this In short, here are the steps, git branch <branchname> git status git checkout <branchname> #To go into the new branch git push origin <branchname>","title":"How to add branches in a git repository?"},{"location":"linux/git/#reference","text":"http://rogerdudler.github.io/git-guide/ https://bitbucket.org/dasarpmar/tutorial/overview https://www.atlassian.com/git/tutorials/setting-up-a-repository/git-init","title":"Reference"},{"location":"linux/grep_sed_awk/","text":"Grep and Egrep commands -i [ignore case] -v [invert the match] -o [only the matching] -A [num] [print the [num] lines after the match] -B [num] [print the [num] lines before the match] First grep something from a file and then find the last line from that grepped file. find FeSe.scf |xargs grep FGL002 |tail -1 Finds FGL002 in file .scf and returns the last line Find only specific numbers from a line less log.ctqmc | egrep -o \"<Sz>=-?[0-9].[0-9]+\" | egrep -oe \u2019-?[0-9].[0-9]+\u2019 > sz_data Find only the numbers from the grepped entity. use regular expression. egrep is for extended grep. find FeSe.scf |xargs grep :FER |tail -1| egrep -o [0-9].[0-9]+ Regular Expressions ^[-+]?[0-9]*\\.?[0-9]+$ ^ - start of string $ - the end of string [0-9]? - 0 or 1 sign indicator [0-9]* - 0 or more integers [0-9]+ - 1 or more integers \\. - the charcter (. is used in regex to mean \"any character\") Sed and Awk Commands p print d delete s substitute a append i insert c change y transform q quit !! Things to note $ represents the last line ^$ represents blank line. sed Syntax sed [-n] [-e] ['command'] [file] -n only prints line specified with the print command -e The next argument is a filename containing editing commands print Command p [ p rint only certain lines] Syntax: [address1[,address2]]p Examples: sed -n '5,10p' file.dat Print lines from line 5 to line 10 sed -n '5p' file.dat Print only 5th line sed -n '5,$p' file.dat Print lines from 5th to last line delete Command d [ d elete certain line] Syntax: [address1[,address2]]d !!Examples: sed '5d' file.dat [Delete line 5 while printing] sed '5,10d' file.dat [Delete lines from 5th to 10th while printing] sed '/^$d' file.dat [Delete line 5 while printing] sed '/^$/,$d' file.dat [Delete from first blank line through last line.] sed '/str/,$d' file.dat [Delete all lines after the line containing string **str**. [Also has to be single quotation mark.] Substitute Command s [ s ubstitute AA by BB ] Syntax: [address(es)]s/pattern/replacement/[flags] flags: n [nth occurrence] : g [global occurrence] Examples: sed \u2018s/ AA/ BB/\u2019 file.dat Only the first occurrence sed \u2018s/ AA/ BB/g\u2019 file.dat global (all) occurrences sed -i \u2018s/ AA/ BB/g\u2019 file.dat replace in file (all) occurrences For a pattern which occurs multiple times in a file, replace only some of them. [Remember sig.inp for ferromagnetic calculations] sed 's/AA/BB/2' Replace only 2nd occurrence sed 's/AA/BB/2g' Replace all after 2nd occurrence !!Note: while using sed in bash shell, be careful about '{text}' and \"{text}\" the later has been useful. Learn when to use one and when the other. More information can be on A great course for learning UNIX http://www.cs.nyu.edu/~mohri/unix08/ Basic Unix commands http://unixcommandstutorial.blogspot.com/2013/04/unixawkperl-sqlplsql-examples.html some tutorials on awk http://www.grymoire.com/Unix/Awk.html#uh-14 awk awk can be as useful as sed and in many cases even better. print only some columns of some output awk '{print $2}' file.txt [print 2nd column] grep 'str' file.txt |awk '{print $2}' [print 2nd column of the output] grep 'str' file.txt |awk '{print $2 $3}' [print 2nd and 3rd column] print only some rows from file awk 'NR==1{print}' file.txt [print 1st line] awk 'NR==1;END{print}' file.txt [print first and last line.] awk 'NR==1;END{print $2}' file.txt [print 2nd column of first and last line.] find pattern and print only certain column awk '/pattern/{print $1}' file [find lines with pattern and print first column] !Sed/Awk inside a bash script Insied a bash script you have to be little careful using sed and awk, especially if you are using a variable inside a sed command. For example the following command inside a bash script sed -i 's/${var1}/${var2}' filename.dat won't work . First thing is it needs double quotes ** as opposed to single quotes ' and the second thing is the / sign must be replaced by apersand symbol & . So the correct command would be sed -i **s&${var1}&${var2}&g** filename.dat Sed deleting last n lines of a file Option1: delete one line at a time for n times. very expensive for large files. for i in `seq 1 n`; do sed -i '${d}' file.txt Option2: print the file in reverse using tac and cut the first n lines and reverse the file once again using tac . tac file.txt |sed '1,n{d}' |tac > file_tmp mv file_tmp file.txt Option3: Found in the webpage sedoneliner sed -i -e :a -e '$d;N;2,4ba' -e 'P;D' test_file Some other nice comands sed -i -e \"1,${n0}d\" $f # works to delete lines from 1 to n0 sed -i -e \"${n0},${n1}d\" $f # deletes lines from n0 to n1","title":"grep/sed/awk commands"},{"location":"linux/grep_sed_awk/#grep-and-egrep","text":"","title":"Grep and Egrep"},{"location":"linux/grep_sed_awk/#commands","text":"-i [ignore case] -v [invert the match] -o [only the matching] -A [num] [print the [num] lines after the match] -B [num] [print the [num] lines before the match] First grep something from a file and then find the last line from that grepped file. find FeSe.scf |xargs grep FGL002 |tail -1 Finds FGL002 in file .scf and returns the last line Find only specific numbers from a line less log.ctqmc | egrep -o \"<Sz>=-?[0-9].[0-9]+\" | egrep -oe \u2019-?[0-9].[0-9]+\u2019 > sz_data Find only the numbers from the grepped entity. use regular expression. egrep is for extended grep. find FeSe.scf |xargs grep :FER |tail -1| egrep -o [0-9].[0-9]+","title":"commands"},{"location":"linux/grep_sed_awk/#regular-expressions","text":"^[-+]?[0-9]*\\.?[0-9]+$ ^ - start of string $ - the end of string [0-9]? - 0 or 1 sign indicator [0-9]* - 0 or more integers [0-9]+ - 1 or more integers \\. - the charcter (. is used in regex to mean \"any character\")","title":"Regular Expressions"},{"location":"linux/grep_sed_awk/#sed-and-awk","text":"","title":"Sed and Awk"},{"location":"linux/grep_sed_awk/#commands_1","text":"p print d delete s substitute a append i insert c change y transform q quit !! Things to note $ represents the last line ^$ represents blank line.","title":"Commands"},{"location":"linux/grep_sed_awk/#sed-syntax","text":"sed [-n] [-e] ['command'] [file] -n only prints line specified with the print command -e The next argument is a filename containing editing commands","title":"sed Syntax"},{"location":"linux/grep_sed_awk/#print","text":"Command p [ p rint only certain lines] Syntax: [address1[,address2]]p Examples: sed -n '5,10p' file.dat Print lines from line 5 to line 10 sed -n '5p' file.dat Print only 5th line sed -n '5,$p' file.dat Print lines from 5th to last line","title":"print"},{"location":"linux/grep_sed_awk/#delete","text":"Command d [ d elete certain line] Syntax: [address1[,address2]]d !!Examples: sed '5d' file.dat [Delete line 5 while printing] sed '5,10d' file.dat [Delete lines from 5th to 10th while printing] sed '/^$d' file.dat [Delete line 5 while printing] sed '/^$/,$d' file.dat [Delete from first blank line through last line.] sed '/str/,$d' file.dat [Delete all lines after the line containing string **str**. [Also has to be single quotation mark.]","title":"delete"},{"location":"linux/grep_sed_awk/#substitute","text":"Command s [ s ubstitute AA by BB ] Syntax: [address(es)]s/pattern/replacement/[flags] flags: n [nth occurrence] : g [global occurrence]","title":"Substitute"},{"location":"linux/grep_sed_awk/#examples","text":"sed \u2018s/ AA/ BB/\u2019 file.dat Only the first occurrence sed \u2018s/ AA/ BB/g\u2019 file.dat global (all) occurrences sed -i \u2018s/ AA/ BB/g\u2019 file.dat replace in file (all) occurrences For a pattern which occurs multiple times in a file, replace only some of them. [Remember sig.inp for ferromagnetic calculations] sed 's/AA/BB/2' Replace only 2nd occurrence sed 's/AA/BB/2g' Replace all after 2nd occurrence !!Note: while using sed in bash shell, be careful about '{text}' and \"{text}\" the later has been useful. Learn when to use one and when the other. More information can be on A great course for learning UNIX http://www.cs.nyu.edu/~mohri/unix08/ Basic Unix commands http://unixcommandstutorial.blogspot.com/2013/04/unixawkperl-sqlplsql-examples.html some tutorials on awk http://www.grymoire.com/Unix/Awk.html#uh-14","title":"Examples:"},{"location":"linux/grep_sed_awk/#awk","text":"awk can be as useful as sed and in many cases even better. print only some columns of some output awk '{print $2}' file.txt [print 2nd column] grep 'str' file.txt |awk '{print $2}' [print 2nd column of the output] grep 'str' file.txt |awk '{print $2 $3}' [print 2nd and 3rd column] print only some rows from file awk 'NR==1{print}' file.txt [print 1st line] awk 'NR==1;END{print}' file.txt [print first and last line.] awk 'NR==1;END{print $2}' file.txt [print 2nd column of first and last line.] find pattern and print only certain column awk '/pattern/{print $1}' file [find lines with pattern and print first column] !Sed/Awk inside a bash script Insied a bash script you have to be little careful using sed and awk, especially if you are using a variable inside a sed command. For example the following command inside a bash script sed -i 's/${var1}/${var2}' filename.dat won't work . First thing is it needs double quotes ** as opposed to single quotes ' and the second thing is the / sign must be replaced by apersand symbol & . So the correct command would be sed -i **s&${var1}&${var2}&g** filename.dat","title":"awk"},{"location":"linux/grep_sed_awk/#sed-deleting-last-n-lines-of-a-file","text":"Option1: delete one line at a time for n times. very expensive for large files. for i in `seq 1 n`; do sed -i '${d}' file.txt Option2: print the file in reverse using tac and cut the first n lines and reverse the file once again using tac . tac file.txt |sed '1,n{d}' |tac > file_tmp mv file_tmp file.txt Option3: Found in the webpage sedoneliner sed -i -e :a -e '$d;N;2,4ba' -e 'P;D' test_file Some other nice comands sed -i -e \"1,${n0}d\" $f # works to delete lines from 1 to n0 sed -i -e \"${n0},${n1}d\" $f # deletes lines from n0 to n1","title":"Sed deleting last n lines of a file"},{"location":"linux/vi/","text":"VI editor basic commands Vi Editor basically has two modes: INSERT MODE and COMMAND MODE . In Insert mode, one can edit, make changes. On the other hand in command mode, we can enter commands to perform certain task, for example substitution, deletion. To switch from INSERT MODE to COMMAND MODE press Esc key. To switch from COMMAND MODE to INSERT MODE press I . Basics commands i : Insert text at the current cursor position I : Insert text at the begining of the current line. o : Add text in a new line below the current line. O : Add text in a new line above the current line. a : Append text after the position of cursor. A : Append at the end of line. Move Cursor h : Move the cursor one character to the left l : Move the cursor one character to the right k : Move the cursor up one line j : Move the cursor down one line gg : Go to the begining of file. G : Go to the end of the file. ^d : Shift one page down [^: CTRL] ^u : Shift one page up [^: CTRL] $ : Move cursor to the end of current line 0 : Move cursor to the beginning of current line w : Forward one word b : Backward one word Cut / Copy / Paste Delete x : Delete a character dd : Delete current line d$ : Delete from the position of the cursor to end of the line dw : Delete word from cursor on db : Delete word backward Copy (Yank) yy : Yank current line y$ : Yank to end of current line from cursor yw : Yank from cursor to end of current word 2yy : Yank 2 lines y : Yanks the Highlighted characters. Paste p : Paste below the cursor P : Paste above the cursor u : Undo last change U : Restore line J : Join next line down to the end of the current line Highlight characters v : activates the visual mode and one can move cursor to highlight required characters. ^v : Highlight vertically. Very useful for inserting tab in multiple lines. Exit from the Editor :wq : Write file to disk and quit the editor :q! : Quit (Without saving the changes) :q : Quit (Will print out a warning if changes have been made) :w abc : Save the current file to a new file abc more commands Find and Replace One of the widely used commands for me personally. :[range]s/{pattern}/{string}/[flags] [count] Possible Flags: [c] : Confirm each substitution [g] : Replace all occurrrences in the line. [i] : Ignore case for the pattern s : stands for substitute Some examples :s/XX/YY/g [only in current line] :%s/XX/YY/g [in the entire file] :5,12s/XX/YY/g [only in specific lines 5 and 12] :.,$s/XX/YY/g [current + all the following lines.] :s/XX/YY/g 4 [Current + following 4 lines] :.,+2s/XX/YY/g [current + next 2 lines same as above.] Explanation :s for substitution, . : current line , $ :end line, % : entire file, g : global search but locally global. only for that line. insert spaces or indentation in multiple lines highlight the line[s] and press >> or << deleting certain parts :%normal 2x [Delete the first n characters of every lines] :%s/^ / [Delete the first n characters of every lines only if they are spaces] :%s/\\s+$//e [Delete any trailing space at the end of all the lines] :%s/ :%s/^\\s\\+//e [Delete all initial spaces(or tabs) of all the lines] [^: begining of line; \\s looks for spaces(including tabs); \\+ looks for one and or more occurance; //e: first / means substitute by and nothing after it means nothing and \\e means if not found the string then suppress the error.] :%le [==:left] [Global left alignment (much more efficient)] OPEN/ CLOSE/ SPLIT files vim f1.txt [open single file] vim -o f1.txt f2.txt [open two files in horizontal] vim -O f1.txt f2.txt [open two files vertically] CTRL w [R/L/T/B arrow] [toggle between different split windows] CTRL +w w [ Toggle between different split windows] :e new_file [open new file in this window] :sp new_file [splits Horizontally and opens the new file] :vsp new_file [splits vertically and opens the new file] CTRL +w , s [split Horizontally ] CTRL +w , v [split vertically ] CTRL +w, Q [Quit the current window] g lobal command The global command is useful in many cases. general pattern: :[range]g/pattern/cmd !!Examples :g/[abcde]/d [delete lines matching pattern abcde] :!g/[abcde]/d [delete lines not matching the pattern abcde] :g/^\\s*$/d [^($) means start(end) of line and \\s* means 0 or many spaces including tab, delete all empty lines.] :g/[pat]/s/abc/def/gi [In all lines containing pattern pat, substitute abc by def.] Find a pattern in a line and delete everything after it. or delete after that pattern :1,$s/pattern.*//g . # to delete everything after (and including) that pattern :1,$s/pattern.*/pattern/g . # to delete everything after (excluding) that pattern Some other Examples Sometime you have to comment few lines. For that Usually for bash-script we put a # sign in the front of every line to comment that line. Easier way is following: go to top(bottom) of the lines press CTRL v to go to visual mode and press j or k to go down or up to whatever many lines you need to comment. Press I (capital i) and put a sign # now go to command mode by pressing Esc key and save the file :w . Should work! Playing with tab and spaces in VIM A lot of times I get old scripts and codes where a tab is used and while making changes and compiling the code I get indentation error due to tabs. Here is how to fix this. Replace tab by spaces: :retab Set length of spaces for a tab: :set tabstop=4 , :set shiftwidth=4 , :set expandtab For more info, find my .vimrc file my vimrc file Yank and Paste same thing multiple times. xnoremap p pgvy p to paste, gv to re-select what was originally selected. y to copy it again.","title":"vi commands"},{"location":"linux/vi/#vi-editor-basic-commands","text":"Vi Editor basically has two modes: INSERT MODE and COMMAND MODE . In Insert mode, one can edit, make changes. On the other hand in command mode, we can enter commands to perform certain task, for example substitution, deletion. To switch from INSERT MODE to COMMAND MODE press Esc key. To switch from COMMAND MODE to INSERT MODE press I .","title":"VI editor basic commands"},{"location":"linux/vi/#basics-commands","text":"i : Insert text at the current cursor position I : Insert text at the begining of the current line. o : Add text in a new line below the current line. O : Add text in a new line above the current line. a : Append text after the position of cursor. A : Append at the end of line. Move Cursor h : Move the cursor one character to the left l : Move the cursor one character to the right k : Move the cursor up one line j : Move the cursor down one line gg : Go to the begining of file. G : Go to the end of the file. ^d : Shift one page down [^: CTRL] ^u : Shift one page up [^: CTRL] $ : Move cursor to the end of current line 0 : Move cursor to the beginning of current line w : Forward one word b : Backward one word Cut / Copy / Paste Delete x : Delete a character dd : Delete current line d$ : Delete from the position of the cursor to end of the line dw : Delete word from cursor on db : Delete word backward Copy (Yank) yy : Yank current line y$ : Yank to end of current line from cursor yw : Yank from cursor to end of current word 2yy : Yank 2 lines y : Yanks the Highlighted characters. Paste p : Paste below the cursor P : Paste above the cursor u : Undo last change U : Restore line J : Join next line down to the end of the current line Highlight characters v : activates the visual mode and one can move cursor to highlight required characters. ^v : Highlight vertically. Very useful for inserting tab in multiple lines. Exit from the Editor :wq : Write file to disk and quit the editor :q! : Quit (Without saving the changes) :q : Quit (Will print out a warning if changes have been made) :w abc : Save the current file to a new file abc","title":"Basics commands"},{"location":"linux/vi/#more-commands","text":"Find and Replace One of the widely used commands for me personally. :[range]s/{pattern}/{string}/[flags] [count] Possible Flags: [c] : Confirm each substitution [g] : Replace all occurrrences in the line. [i] : Ignore case for the pattern s : stands for substitute Some examples :s/XX/YY/g [only in current line] :%s/XX/YY/g [in the entire file] :5,12s/XX/YY/g [only in specific lines 5 and 12] :.,$s/XX/YY/g [current + all the following lines.] :s/XX/YY/g 4 [Current + following 4 lines] :.,+2s/XX/YY/g [current + next 2 lines same as above.] Explanation :s for substitution, . : current line , $ :end line, % : entire file, g : global search but locally global. only for that line. insert spaces or indentation in multiple lines highlight the line[s] and press >> or << deleting certain parts :%normal 2x [Delete the first n characters of every lines] :%s/^ / [Delete the first n characters of every lines only if they are spaces] :%s/\\s+$//e [Delete any trailing space at the end of all the lines] :%s/ :%s/^\\s\\+//e [Delete all initial spaces(or tabs) of all the lines] [^: begining of line; \\s looks for spaces(including tabs); \\+ looks for one and or more occurance; //e: first / means substitute by and nothing after it means nothing and \\e means if not found the string then suppress the error.] :%le [==:left] [Global left alignment (much more efficient)] OPEN/ CLOSE/ SPLIT files vim f1.txt [open single file] vim -o f1.txt f2.txt [open two files in horizontal] vim -O f1.txt f2.txt [open two files vertically] CTRL w [R/L/T/B arrow] [toggle between different split windows] CTRL +w w [ Toggle between different split windows] :e new_file [open new file in this window] :sp new_file [splits Horizontally and opens the new file] :vsp new_file [splits vertically and opens the new file] CTRL +w , s [split Horizontally ] CTRL +w , v [split vertically ] CTRL +w, Q [Quit the current window] g lobal command The global command is useful in many cases. general pattern: :[range]g/pattern/cmd !!Examples :g/[abcde]/d [delete lines matching pattern abcde] :!g/[abcde]/d [delete lines not matching the pattern abcde] :g/^\\s*$/d [^($) means start(end) of line and \\s* means 0 or many spaces including tab, delete all empty lines.] :g/[pat]/s/abc/def/gi [In all lines containing pattern pat, substitute abc by def.] Find a pattern in a line and delete everything after it. or delete after that pattern :1,$s/pattern.*//g . # to delete everything after (and including) that pattern :1,$s/pattern.*/pattern/g . # to delete everything after (excluding) that pattern Some other Examples Sometime you have to comment few lines. For that Usually for bash-script we put a # sign in the front of every line to comment that line. Easier way is following: go to top(bottom) of the lines press CTRL v to go to visual mode and press j or k to go down or up to whatever many lines you need to comment. Press I (capital i) and put a sign # now go to command mode by pressing Esc key and save the file :w . Should work!","title":"more commands"},{"location":"linux/vi/#playing-with-tab-and-spaces-in-vim","text":"A lot of times I get old scripts and codes where a tab is used and while making changes and compiling the code I get indentation error due to tabs. Here is how to fix this. Replace tab by spaces: :retab Set length of spaces for a tab: :set tabstop=4 , :set shiftwidth=4 , :set expandtab For more info, find my .vimrc file my vimrc file","title":"Playing with tab and spaces in VIM"},{"location":"linux/vi/#yank-and-paste-same-thing-multiple-times","text":"xnoremap p pgvy p to paste, gv to re-select what was originally selected. y to copy it again.","title":"Yank and Paste same thing multiple times."},{"location":"ml/chest_xray/","text":"Chest X-Ray Medical Diagnosis with Deep Learning Welcome to the first assignment of course 1! In this assignment! You will explore medical image diagnosis by building a state-of-the-art chest X-ray classifier using Keras. The assignment will walk through some of the steps of building and evaluating this deep learning classifier model. In particular, you will: - Pre-process and prepare a real-world X-ray dataset - Use transfer learning to retrain a DenseNet model for X-ray image classification - Learn a technique to handle class imbalance - Measure diagnostic performance by computing the AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve - Visualize model activity using GradCAMs In completing this assignment you will learn about the following topics: Data preparation Visualizing data Preventing data leakage Model Development Addressing class imbalance Leveraging pre-trained models using transfer learning Evaluation AUC and ROC curves Outline Use these links to jump to specific sections of this assignment! 1. Import Packages and Function 2. Load the Datasets 2.1 Preventing Data Leakage Exercise 1 - Checking Data Leakage 2.2 Preparing Images 3. Model Development 3.1 Addressing Class Imbalance Exercise 2 - Computing Class Frequencies Exercise 3 - Weighted Loss 3.3 DenseNet121 4. Training [optional] 4.1 Training on the Larger Dataset 5. Prediction and Evaluation 5.1 ROC Curve and AUROC 5.2 Visualizing Learning with GradCAM 1. Import Packages and Functions\u00b6 We'll make use of the following packages: - numpy and pandas is what we'll use to manipulate our data - matplotlib.pyplot and seaborn will be used to produce plots for visualization - util will provide the locally defined utility functions that have been provided for this assignment We will also use several modules from the keras framework for building deep learning models. Run the next cell to import all the necessary packages. # Import necessary packages import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline import os import seaborn as sns from keras.preprocessing.image import ImageDataGenerator from keras.applications.densenet import DenseNet121 from keras.layers import Dense, GlobalAveragePooling2D from keras.models import Model, load_model from keras import backend as K import week1_util sns . set_style( 'whitegrid' ) data_dir = \"../../../dataset/\" Using TensorFlow backend. 2 Load the Datasets For this assignment, we will be using the ChestX-ray8 dataset which contains 108,948 frontal-view X-ray images of 32,717 unique patients. - Each image in the data set contains multiple text-mined labels identifying 14 different pathological conditions. - These in turn can be used by physicians to diagnose 8 different diseases. - We will use this data to develop a single model that will provide binary classification predictions for each of the 14 labeled pathologies. - In other words it will predict 'positive' or 'negative' for each of the pathologies. You can download the entire dataset for free here . - We have provided a ~1000 image subset of the images for you. - These can be accessed in the folder path stored in the IMAGE_DIR variable. The dataset includes a CSV file that provides the labels for each X-ray. To make your job a bit easier, we have processed the labels for our small sample and generated three new files to get you started. These three files are: nih/train-small.csv : 875 images from our dataset to be used for training. nih/valid-small.csv : 109 images from our dataset to be used for validation. nih/test.csv : 420 images from our dataset to be used for testing. This dataset has been annotated by consensus among four different radiologists for 5 of our 14 pathologies: - Consolidation - Edema - Effusion - Cardiomegaly - Atelectasis Sidebar on meaning of 'class' It is worth noting that the word 'class' is used in multiple ways is these discussions. - We sometimes refer to each of the 14 pathological conditions that are labeled in our dataset as a class. - But for each of those pathologies we are attempting to predict whether a certain condition is present (i.e. positive result) or absent (i.e. negative result). - These two possible labels of 'positive' or 'negative' (or the numerical equivalent of 1 or 0) are also typically referred to as classes. - Moreover, we also use the term in reference to software code 'classes' such as ImageDataGenerator . As long as you are aware of all this though, it should not cause you any confusion as the term 'class' is usually clear from the context in which it is used. Read in the data Let's open these files using the pandas library train_df = pd . read_csv(data_dir + \"nih/train-small.csv\" ) valid_df = pd . read_csv(data_dir + \"nih/valid-small.csv\" ) test_df = pd . read_csv(data_dir + \"nih/test.csv\" ) print ( f\"train_df.shape: { train_df . shape } \" ) print ( f\"valid_df.shape: { valid_df . shape } \" ) print ( f\"test_df.shape: { test_df . shape } \" ) train_df.shape: (1000, 16) valid_df.shape: (109, 16) test_df.shape: (420, 16) labels = list (train_df . keys()) labels . remove( 'Image' ) labels . remove( 'PatientId' ) print (labels) # quick check #labs = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Edema', # 'Atelectasis', 'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Consolidation'] #print ([i in labs for i in labels]) #labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', # 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'] ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'] 2.1 Preventing Data Leakage It is worth noting that our dataset contains multiple images for each patient. This could be the case, for example, when a patient has taken multiple X-ray images at different times during their hospital visits. In our data splitting, we have ensured that the split is done on the patient level so that there is no data \"leakage\" between the train, validation, and test datasets. Exercise 1 - Checking Data Leakage In the cell below, write a function to check whether there is leakage between two datasets. We'll use this to make sure there are no patients in the test set that are also present in either the train or validation sets. d1 = [ 0 , 1 , 2 , 5 ] d2 = [ 5 , 3 , 4 ] arr = [i in d2 for i in d1] l1 = any ( arr ) print (arr, l1) [False, False, False, True] True # UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) def check_for_leakage (df1, df2, patient_col): \"\"\" Return True if there any patients are in both df1 and df2. Args: df1 (dataframe): dataframe describing first dataset df2 (dataframe): dataframe describing second dataset patient_col (str): string name of column with patient IDs Returns: leakage (bool): True if there is leakage, otherwise False \"\"\" leakage = any ([i in df1[patient_col] . values for i in df2[patient_col] . values ] ) # .values is important here return leakage Run the next cell to check if there are patients in both train and test or in both valid and test. print ( \"leakage between train and test: {} \" . format(check_for_leakage(train_df, test_df, 'PatientId' ))) print ( \"leakage between valid and test: {} \" . format(check_for_leakage(valid_df, test_df, 'PatientId' ))) leakage between train and test: False leakage between valid and test: False If we get False for both, then we're ready to start preparing the datasets for training. Remember to always check for data leakage! 2.2 Preparing Images With our dataset splits ready, we can now proceed with setting up our model to consume them. - For this we will use the off-the-shelf ImageDataGenerator class from the Keras framework, which allows us to build a \"generator\" for images specified in a dataframe. - This class also provides support for basic data augmentation such as random horizontal flipping of images. - We also use the generator to transform the values in each batch so that their mean is $0$ and their standard deviation is 1. - This will facilitate model training by standardizing the input distribution. - The generator also converts our single channel X-ray images (gray-scale) to a three-channel format by repeating the values in the image across all channels. - We will want this because the pre-trained model that we'll use requires three-channel inputs. Since it is mainly a matter of reading and understanding Keras documentation, we have implemented the generator for you. There are a few things to note: 1. We normalize the mean and standard deviation of the data 3. We shuffle the input after each epoch. 4. We set the image size to be 320px by 320px def get_train_generator (df, image_dir, x_col, y_cols, shuffle = True , batch_size = 8 , seed = 1 , target_w = 320 , target_h = 320 ): \"\"\" Return generator for training set, normalizing using batch statistics. Args: train_df (dataframe): dataframe specifying training data. image_dir (str): directory where image files are held. x_col (str): name of column in df that holds filenames. y_cols (list): list of strings that hold y labels for images. batch_size (int): images per batch to be fed into model during training. seed (int): random seed. target_w (int): final width of input images. target_h (int): final height of input images. Returns: train_generator (DataFrameIterator): iterator over training set \"\"\" print ( \"getting train generator...\" ) # normalize images # (x-mu)/sigma image_generator = ImageDataGenerator( samplewise_center = True , # for (x-mu) samplewise_std_normalization = True ) # (x-mu)/sigma # flow from directory with specified batch size # and target image size generator = image_generator . flow_from_dataframe( dataframe = df, directory = image_dir, x_col = x_col, y_col = y_cols, class_mode = \"raw\" , batch_size = batch_size, shuffle = shuffle, seed = seed, target_size = (target_w,target_h)) return generator Build a separate generator for valid and test sets Now we need to build a new generator for validation and testing data. Why can't we use the same generator as for the training data? Look back at the generator we wrote for the training data. - It normalizes each image per batch , meaning that it uses batch statistics. - We should not do this with the test and validation data, since in a real life scenario we don't process incoming images a batch at a time (we process one image at a time). - Knowing the average per batch of test data would effectively give our model an advantage. - The model should not have any information about the test data. What we need to do is normalize incoming test data using the statistics computed from the training set . * We implement this in the function below. * There is one technical note. Ideally, we would want to compute our sample mean and standard deviation using the entire training set. * However, since this is extremely large, that would be very time consuming. * In the interest of time, we'll take a random sample of the dataset and calcualte the sample mean and sample standard deviation. def get_test_and_valid_generator (valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size = 100 , batch_size = 8 , seed = 1 , target_w = 320 , target_h = 320 ): \"\"\" Return generator for validation set and test test set using normalization statistics from training set. Args: valid_df (dataframe): dataframe specifying validation data. test_df (dataframe): dataframe specifying test data. train_df (dataframe): dataframe specifying training data. image_dir (str): directory where image files are held. x_col (str): name of column in df that holds filenames. y_cols (list): list of strings that hold y labels for images. sample_size (int): size of sample to use for normalization statistics. batch_size (int): images per batch to be fed into model during training. seed (int): random seed. target_w (int): final width of input images. target_h (int): final height of input images. Returns: test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively \"\"\" print ( \"getting train and valid generators...\" ) # get generator to sample dataset raw_train_generator = ImageDataGenerator() . flow_from_dataframe( dataframe = train_df, directory = IMAGE_DIR, x_col = \"Image\" , y_col = labels, class_mode = \"raw\" , batch_size = sample_size, shuffle = True , target_size = (target_w, target_h)) # get data sample batch = raw_train_generator . next() data_sample = batch[ 0 ] # use sample to fit mean and std for test set generator image_generator = ImageDataGenerator( featurewise_center = True , featurewise_std_normalization = True ) # fit generator to sample from training data image_generator . fit(data_sample) # get test generator valid_generator = image_generator . flow_from_dataframe( dataframe = valid_df, directory = image_dir, x_col = x_col, y_col = y_cols, class_mode = \"raw\" , batch_size = batch_size, shuffle = False , seed = seed, target_size = (target_w,target_h)) test_generator = image_generator . flow_from_dataframe( dataframe = test_df, directory = image_dir, x_col = x_col, y_col = y_cols, class_mode = \"raw\" , batch_size = batch_size, shuffle = False , seed = seed, target_size = (target_w,target_h)) return valid_generator, test_generator With our generator function ready, let's make one generator for our training data and one each of our test and validation datasets. IMAGE_DIR = data_dir + \"nih/images-small/\" train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\" , labels) valid_generator, test_generator = get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"Image\" , labels) getting train generator... Found 1000 validated image filenames. getting train and valid generators... Found 1000 validated image filenames. Found 1 validated image filenames. Found 420 validated image filenames. /usr/local/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 108 invalid image filename(s) in x_col=\"Image\". These filename(s) will be ignored. .format(n_invalid, x_col) Let's peek into what the generator gives our model during training and validation. We can do this by calling the __get_item__(index) function: x, y = train_generator . __getitem__ ( 0 ) #print (y) plt . imshow(x[ 0 ]); Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). print ( f\"x.shape= { x . shape } \\n y.shape= { y . shape } \" ) x.shape= (8, 320, 320, 3) y.shape= (8, 14) 3 Model Development Now we'll move on to model training and development. We have a few practical challenges to deal with before actually training a neural network, though. The first is class imbalance. 3.1 Addressing Class Imbalance One of the challenges with working with medical diagnostic datasets is the large class imbalance present in such datasets. Let's plot the frequency of each of the labels in our dataset: values = np . mean(train_generator . labels, axis = 0 ) sns . barplot(values, labels, order = labels) plt . yticks(fontsize = 13 ); plt . title( \"Frequency of Each Class\" , fontsize = 14 ); We can see from this plot that the prevalance of positive cases varies significantly across the different pathologies. (These trends mirror the ones in the full dataset as well.) * The Hernia pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. * But even the Infiltration pathology, which has the least amount of imbalance, has only 17.5% of the training cases labelled positive. Ideally, we would train our model using an evenly balanced dataset so that the positive and negative training cases would contribute equally to the loss. If we use a normal cross-entropy loss function with a highly unbalanced dataset, as we are seeing here, then the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss. Impact of class imbalance on loss function Let's take a closer look at this. Assume we would have used a normal cross-entropy loss for each pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is: $$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$ where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive. Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms contributes to the loss (the other term is multiplied by zero, and becomes zero). We can rewrite the overall average cross-entropy loss over the entire training set $\\mathcal{D}$ of size $N$ as follows: $$\\mathcal{L} {cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum {\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$ Using this formulation, we can see that if there is a large imbalance with very few positive training cases, for example, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: $$freq_{p} = \\frac{\\text{number of positive examples}}{N} $$ $$\\text{and}$$ $$freq_{n} = \\frac{\\text{number of negative examples}}{N}.$$ Exercise 2 - Computing Class Frequencies Complete the function below to calculate these frequences for each label in our dataset. # UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) def compute_class_freqs (labels): \"\"\" Compute positive and negative frequences for each class. Args: labels (np.array): matrix of labels, size (num_examples, num_classes) Returns: positive_frequencies (np.array): array of positive frequences for each class, size (num_classes) negative_frequencies (np.array): array of negative frequences for each class, size (num_classes) \"\"\" N = labels . shape[ 0 ] positive_frequencies = np . sum(labels, axis = 0 ) / N negative_frequencies = 1. - positive_frequencies return positive_frequencies, negative_frequencies Now we'll compute frequencies for our training data. freq_pos, freq_neg = compute_class_freqs(train_generator . labels) freq_pos array([0.106, 0.02 , 0.033, 0.016, 0.128, 0.013, 0.014, 0.002, 0.175, 0.045, 0.054, 0.021, 0.01 , 0.038]) Let's visualize these two contribution ratios next to each other for each of the pathologies: data = pd . DataFrame( { 'Class' : labels, \"Positive_freq\" :freq_pos, \"Negative_freq\" :freq_neg } ) data . plot . bar(x = \"Class\" , y = [ \"Positive_freq\" , \"Negative_freq\" ], figsize = ( 15 , 4 ), color = [ 'blue' , 'orange' ]); plt . yticks(fontsize = 16 ); plt . xticks(fontsize = 16 ); plt . legend(fontsize = 16 ); The same bar plot can be plotted with seaborn in following way but remember sns barplot requires you to have a single row with multiple labels. For example: one column with Lables with values : positive and negative. and another column with values. Then we can plot the two labels with two barplots. data = pd . DataFrame({ \"Class\" : labels, \"Label\" : \"Positive\" , \"Value\" : freq_pos}) data = data . append([{ \"Class\" : labels[l], \"Label\" : \"Negative\" , \"Value\" : v} for l,v in enumerate (freq_neg)], ignore_index = True ) data . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Class Label Value 0 Atelectasis Positive 0.106 1 Cardiomegaly Positive 0.020 2 Consolidation Positive 0.033 3 Edema Positive 0.016 4 Effusion Positive 0.128 plt . figure(figsize = ( 15 , 4 )) sns . barplot(x = \"Class\" , y = \"Value\" , hue = \"Label\" ,data = data); plt . xticks(rotation = 90 ); plt . yticks(fontsize = 16 ); plt . xticks(fontsize = 16 ); plt . legend(fontsize = 16 ); As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. To have this, we want $$w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n},$$ which we can do simply by taking $$w_{pos} = freq_{neg}$$ $$w_{neg} = freq_{pos}$$ This way, we will be balancing the contribution of positive and negative labels. pos_weights = freq_neg neg_weights = freq_pos pos_contribution = freq_pos * pos_weights neg_contribution = freq_neg * neg_weights Let's verify this by graphing the two contributions next to each other again: data = pd . DataFrame({ \"Class\" : labels, \"Label\" : \"Positive\" , \"Value\" : pos_contribution}) data = data . append([{ \"Class\" : labels[l], \"Label\" : \"Negative\" , \"Value\" : v} for l,v in enumerate (neg_contribution)], ignore_index = True ) plt . figure(figsize = ( 12 , 4 )) plt . xticks(rotation = 90 ) sns . barplot(x = \"Class\" , y = \"Value\" , hue = \"Label\" ,data = data); plt . yticks(fontsize = 16 ); plt . xticks(fontsize = 16 ); plt . legend(fontsize = 16 ); As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let's implement such a loss function. After computing the weights, our final weighted loss for each training case will be $$ \\mathcal{L} {cross-entropy}^{w}(x) = - (w {p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ) $$ Exercise 3 - Weighted Loss Fill out the weighted_loss function below to return a loss function that calculates the weighted loss for each batch. Recall that for the multi-class loss, we add up the average loss for each individual class. Note that we also want to add a small value, $\\epsilon$, to the predicted values before taking their logs. This is simply to avoid a numerical error that would otherwise occur if the predicted value happens to be zero. Note Please use Keras functions to calculate the mean and the log. Keras.mean Keras.log Note on converting the TF-1 code to TF-2 First remove the sess = K.get_session(); with sess.as_default() as sess: line. Replace variable.eval() by variable.numpy() # UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) def get_weighted_loss (pos_weights, neg_weights, epsilon = 1e-7 ): \"\"\" Return weighted loss function given negative weights and positive weights. Args: pos_weights (np.array): array of positive weights for each class, size (num_classes) neg_weights (np.array): array of negative weights for each class, size (num_classes) Returns: weighted_loss (function): weighted loss function \"\"\" def weighted_loss (y_true, y_pred): \"\"\" Return weighted loss value. Args: y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes) y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes) Returns: loss (Float): overall scalar loss summed across all classes \"\"\" # initialize loss to zero loss = 0.0 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ### for i in range ( len (pos_weights)): # for each class, add average weighted loss for that class loss += K . mean( - (pos_weights[i] * y_true[:,i] * K . log(y_pred[:,i] + epsilon) + neg_weights[i] * ( 1 - y_true[:,i]) * K . log( 1 - y_pred[:,i] + epsilon))) return loss return weighted_loss Now let's test our function with some simple cases. 3.3 DenseNet121 Next, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it: 1. A GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. 2. A Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function. # create the base pre-trained model base_model = DenseNet121(weights = './nih/densenet.hdf5' , include_top = False ) x = base_model . output # add a global spatial average pooling layer x = GlobalAveragePooling2D()(x) # and a logistic layer predictions = Dense( len (labels), activation = \"sigmoid\" )(x) model = Model(inputs = base_model . input, outputs = predictions) model . compile(optimizer = 'adam' , loss = get_weighted_loss(pos_weights, neg_weights)) 4 Training [optional] With our model ready for training, we will use the model.fit() function in Keras to train our model. - We are training on a small subset of the dataset (~1%). - So what we care about at this point is to make sure that the loss on the training set is decreasing. Since training can take a considerable time, for pedagogical purposes we have chosen not to train the model here but rather to load a set of pre-trained weights in the next section. However, you can use the code shown below to practice training the model locally on your machine or in Colab. NOTE: Do not run the code below on the Coursera platform as it will exceed the platform's memory limitations. Python Code for training the model: history = model . fit_generator(train_generator, validation_data = valid_generator, steps_per_epoch = 100 , validation_steps = 25 , epochs = 3 ) plt . plot(history . history[ 'loss' ]) plt . ylabel( \"loss\" ) plt . xlabel( \"epoch\" ) plt . title( \"Training Loss Curve\" ) plt . show() 4.1 Training on the Larger Dataset Given that the original dataset is 40GB+ in size and the training process on the full dataset takes a few hours, we have trained the model on a GPU-equipped machine for you and provided the weights file from our model (with a batch size of 32 instead) to be used for the rest of this assignment. The model architecture for our pre-trained model is exactly the same, but we used a few useful Keras \"callbacks\" for this training. Do spend time to read about these callbacks at your leisure as they will be very useful for managing long-running training sessions: You can use ModelCheckpoint callback to monitor your model's val_loss metric and keep a snapshot of your model at the point. You can use the TensorBoard to use the Tensorflow Tensorboard utility to monitor your runs in real-time. You can use the ReduceLROnPlateau to slowly decay the learning rate for your model as it stops getting better on a metric such as val_loss to fine-tune the model in the final steps of training. You can use the EarlyStopping callback to stop the training job when your model stops getting better in it's validation loss. You can set a patience value which is the number of epochs the model does not improve after which the training is terminated. This callback can also conveniently restore the weights for the best metric at the end of training to your model. You can read about these callbacks and other useful Keras callbacks here . Let's load our pre-trained weights into the model now: model . load_weights( \"./nih/pretrained_model.h5\" ) 5 Prediction and Evaluation Now that we have a model, let's evaluate it using our test set. We can conveniently use the predict_generator function to generate the predictions for the images in our test set. Note: The following cell can take about 4 minutes to run. predicted_vals = model . predict_generator(test_generator, steps = len (test_generator)) 5.1 ROC Curve and AUROC We'll cover topic of model evaluation in much more detail in later weeks, but for now we'll walk through computing a metric called the AUC (Area Under the Curve) from the ROC ( Receiver Operating Characteristic ) curve. This is also referred to as the AUROC value, but you will see all three terms in reference to the technique, and often used almost interchangeably. For now, what you need to know in order to interpret the plot is that a curve that is more to the left and the top has more \"area\" under it, and indicates that the model is performing better. We will use the util.get_roc_curve() function which has been provided for you in util.py . Look through this function and note the use of the sklearn library functions to generate the ROC curves and AUROC values for our model. roc_curve roc_auc_score auc_rocs = util . get_roc_curve(labels, predicted_vals, test_generator) You can compare the performance to the AUCs reported in the original ChexNeXt paper in the table below: For reference, here's the AUC figure from the ChexNeXt paper which includes AUC values for their model as well as radiologists on this dataset: This method does take advantage of a few other tricks such as self-training and ensembling as well, which can give a significant boost to the performance. For details about the best performing methods and their performance on this dataset, we encourage you to read the following papers: - CheXNet - CheXpert - ChexNeXt 5.2 Visualizing Learning with GradCAM One of the challenges of using deep learning in medicine is that the complex architecture used for neural networks makes them much harder to interpret compared to traditional machine learning models (e.g. linear models). One of the most common approaches aimed at increasing the interpretability of models for computer vision tasks is to use Class Activation Maps (CAM). - Class activation maps are useful for understanding where the model is \"looking\" when classifying an image. In this section we will use a GradCAM's technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition. - This is done by extracting the gradients of each predicted class, flowing into our model's final convolutional layer. Look at the util.compute_gradcam which has been provided for you in util.py to see how this is done with the Keras framework. It is worth mentioning that GradCAM does not provide a full explanation of the reasoning for each classification probability. - However, it is still a useful tool for \"debugging\" our model and augmenting our prediction so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image. First we will load the small training set and setup to look at the 4 classes with the highest performing AUC measures. df = pd . read_csv(data_dir + \"nih/train-small.csv\" ) IMAGE_DIR = data_dir + \"nih/images-small/\" # only show the labels with top 4 AUC labels_to_show = np . take(labels, np . argsort(auc_rocs)[:: - 1 ])[: 4 ] Now let's look at a few specific images. util . compute_gradcam(model, '00008270_015.png' , IMAGE_DIR, df, labels, labels_to_show) util . compute_gradcam(model, '00011355_002.png' , IMAGE_DIR, df, labels, labels_to_show) util . compute_gradcam(model, '00029855_001.png' , IMAGE_DIR, df, labels, labels_to_show) util . compute_gradcam(model, '00005410_000.png' , IMAGE_DIR, df, labels, labels_to_show) Congratulations, you've completed the first assignment of course one! You've learned how to preprocess data, check for data leakage, train a pre-trained model, and evaluate using the AUC. Great work!","title":"Chest X-ray Medical Diagnosis"},{"location":"ml/chest_xray/#chest-x-ray-medical-diagnosis-with-deep-learning","text":"Welcome to the first assignment of course 1! In this assignment! You will explore medical image diagnosis by building a state-of-the-art chest X-ray classifier using Keras. The assignment will walk through some of the steps of building and evaluating this deep learning classifier model. In particular, you will: - Pre-process and prepare a real-world X-ray dataset - Use transfer learning to retrain a DenseNet model for X-ray image classification - Learn a technique to handle class imbalance - Measure diagnostic performance by computing the AUC (Area Under the Curve) for the ROC (Receiver Operating Characteristic) curve - Visualize model activity using GradCAMs In completing this assignment you will learn about the following topics: Data preparation Visualizing data Preventing data leakage Model Development Addressing class imbalance Leveraging pre-trained models using transfer learning Evaluation AUC and ROC curves","title":"Chest X-Ray Medical Diagnosis with Deep Learning"},{"location":"ml/chest_xray/#outline","text":"Use these links to jump to specific sections of this assignment! 1. Import Packages and Function 2. Load the Datasets 2.1 Preventing Data Leakage Exercise 1 - Checking Data Leakage 2.2 Preparing Images 3. Model Development 3.1 Addressing Class Imbalance Exercise 2 - Computing Class Frequencies Exercise 3 - Weighted Loss 3.3 DenseNet121 4. Training [optional] 4.1 Training on the Larger Dataset 5. Prediction and Evaluation 5.1 ROC Curve and AUROC 5.2 Visualizing Learning with GradCAM","title":"Outline"},{"location":"ml/chest_xray/#1-import-packages-and-functions","text":"We'll make use of the following packages: - numpy and pandas is what we'll use to manipulate our data - matplotlib.pyplot and seaborn will be used to produce plots for visualization - util will provide the locally defined utility functions that have been provided for this assignment We will also use several modules from the keras framework for building deep learning models. Run the next cell to import all the necessary packages. # Import necessary packages import pandas as pd import numpy as np import matplotlib.pyplot as plt % matplotlib inline import os import seaborn as sns from keras.preprocessing.image import ImageDataGenerator from keras.applications.densenet import DenseNet121 from keras.layers import Dense, GlobalAveragePooling2D from keras.models import Model, load_model from keras import backend as K import week1_util sns . set_style( 'whitegrid' ) data_dir = \"../../../dataset/\" Using TensorFlow backend.","title":"1. Import Packages and Functions\u00b6"},{"location":"ml/chest_xray/#2-load-the-datasets","text":"For this assignment, we will be using the ChestX-ray8 dataset which contains 108,948 frontal-view X-ray images of 32,717 unique patients. - Each image in the data set contains multiple text-mined labels identifying 14 different pathological conditions. - These in turn can be used by physicians to diagnose 8 different diseases. - We will use this data to develop a single model that will provide binary classification predictions for each of the 14 labeled pathologies. - In other words it will predict 'positive' or 'negative' for each of the pathologies. You can download the entire dataset for free here . - We have provided a ~1000 image subset of the images for you. - These can be accessed in the folder path stored in the IMAGE_DIR variable. The dataset includes a CSV file that provides the labels for each X-ray. To make your job a bit easier, we have processed the labels for our small sample and generated three new files to get you started. These three files are: nih/train-small.csv : 875 images from our dataset to be used for training. nih/valid-small.csv : 109 images from our dataset to be used for validation. nih/test.csv : 420 images from our dataset to be used for testing. This dataset has been annotated by consensus among four different radiologists for 5 of our 14 pathologies: - Consolidation - Edema - Effusion - Cardiomegaly - Atelectasis","title":"2 Load the Datasets"},{"location":"ml/chest_xray/#sidebar-on-meaning-of-class","text":"It is worth noting that the word 'class' is used in multiple ways is these discussions. - We sometimes refer to each of the 14 pathological conditions that are labeled in our dataset as a class. - But for each of those pathologies we are attempting to predict whether a certain condition is present (i.e. positive result) or absent (i.e. negative result). - These two possible labels of 'positive' or 'negative' (or the numerical equivalent of 1 or 0) are also typically referred to as classes. - Moreover, we also use the term in reference to software code 'classes' such as ImageDataGenerator . As long as you are aware of all this though, it should not cause you any confusion as the term 'class' is usually clear from the context in which it is used.","title":"Sidebar on meaning of 'class'"},{"location":"ml/chest_xray/#read-in-the-data","text":"Let's open these files using the pandas library train_df = pd . read_csv(data_dir + \"nih/train-small.csv\" ) valid_df = pd . read_csv(data_dir + \"nih/valid-small.csv\" ) test_df = pd . read_csv(data_dir + \"nih/test.csv\" ) print ( f\"train_df.shape: { train_df . shape } \" ) print ( f\"valid_df.shape: { valid_df . shape } \" ) print ( f\"test_df.shape: { test_df . shape } \" ) train_df.shape: (1000, 16) valid_df.shape: (109, 16) test_df.shape: (420, 16) labels = list (train_df . keys()) labels . remove( 'Image' ) labels . remove( 'PatientId' ) print (labels) # quick check #labs = ['Cardiomegaly', 'Emphysema', 'Effusion', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Edema', # 'Atelectasis', 'Pneumothorax', 'Pleural_Thickening', 'Pneumonia', 'Fibrosis', 'Consolidation'] #print ([i in labs for i in labels]) #labels = ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', # 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax'] ['Atelectasis', 'Cardiomegaly', 'Consolidation', 'Edema', 'Effusion', 'Emphysema', 'Fibrosis', 'Hernia', 'Infiltration', 'Mass', 'Nodule', 'Pleural_Thickening', 'Pneumonia', 'Pneumothorax']","title":"Read in the data"},{"location":"ml/chest_xray/#21-preventing-data-leakage","text":"It is worth noting that our dataset contains multiple images for each patient. This could be the case, for example, when a patient has taken multiple X-ray images at different times during their hospital visits. In our data splitting, we have ensured that the split is done on the patient level so that there is no data \"leakage\" between the train, validation, and test datasets.","title":"2.1 Preventing Data Leakage"},{"location":"ml/chest_xray/#exercise-1-checking-data-leakage","text":"In the cell below, write a function to check whether there is leakage between two datasets. We'll use this to make sure there are no patients in the test set that are also present in either the train or validation sets. d1 = [ 0 , 1 , 2 , 5 ] d2 = [ 5 , 3 , 4 ] arr = [i in d2 for i in d1] l1 = any ( arr ) print (arr, l1) [False, False, False, True] True # UNQ_C1 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) def check_for_leakage (df1, df2, patient_col): \"\"\" Return True if there any patients are in both df1 and df2. Args: df1 (dataframe): dataframe describing first dataset df2 (dataframe): dataframe describing second dataset patient_col (str): string name of column with patient IDs Returns: leakage (bool): True if there is leakage, otherwise False \"\"\" leakage = any ([i in df1[patient_col] . values for i in df2[patient_col] . values ] ) # .values is important here return leakage Run the next cell to check if there are patients in both train and test or in both valid and test. print ( \"leakage between train and test: {} \" . format(check_for_leakage(train_df, test_df, 'PatientId' ))) print ( \"leakage between valid and test: {} \" . format(check_for_leakage(valid_df, test_df, 'PatientId' ))) leakage between train and test: False leakage between valid and test: False If we get False for both, then we're ready to start preparing the datasets for training. Remember to always check for data leakage!","title":"Exercise 1 - Checking Data Leakage"},{"location":"ml/chest_xray/#22-preparing-images","text":"With our dataset splits ready, we can now proceed with setting up our model to consume them. - For this we will use the off-the-shelf ImageDataGenerator class from the Keras framework, which allows us to build a \"generator\" for images specified in a dataframe. - This class also provides support for basic data augmentation such as random horizontal flipping of images. - We also use the generator to transform the values in each batch so that their mean is $0$ and their standard deviation is 1. - This will facilitate model training by standardizing the input distribution. - The generator also converts our single channel X-ray images (gray-scale) to a three-channel format by repeating the values in the image across all channels. - We will want this because the pre-trained model that we'll use requires three-channel inputs. Since it is mainly a matter of reading and understanding Keras documentation, we have implemented the generator for you. There are a few things to note: 1. We normalize the mean and standard deviation of the data 3. We shuffle the input after each epoch. 4. We set the image size to be 320px by 320px def get_train_generator (df, image_dir, x_col, y_cols, shuffle = True , batch_size = 8 , seed = 1 , target_w = 320 , target_h = 320 ): \"\"\" Return generator for training set, normalizing using batch statistics. Args: train_df (dataframe): dataframe specifying training data. image_dir (str): directory where image files are held. x_col (str): name of column in df that holds filenames. y_cols (list): list of strings that hold y labels for images. batch_size (int): images per batch to be fed into model during training. seed (int): random seed. target_w (int): final width of input images. target_h (int): final height of input images. Returns: train_generator (DataFrameIterator): iterator over training set \"\"\" print ( \"getting train generator...\" ) # normalize images # (x-mu)/sigma image_generator = ImageDataGenerator( samplewise_center = True , # for (x-mu) samplewise_std_normalization = True ) # (x-mu)/sigma # flow from directory with specified batch size # and target image size generator = image_generator . flow_from_dataframe( dataframe = df, directory = image_dir, x_col = x_col, y_col = y_cols, class_mode = \"raw\" , batch_size = batch_size, shuffle = shuffle, seed = seed, target_size = (target_w,target_h)) return generator","title":"2.2 Preparing Images"},{"location":"ml/chest_xray/#build-a-separate-generator-for-valid-and-test-sets","text":"Now we need to build a new generator for validation and testing data. Why can't we use the same generator as for the training data? Look back at the generator we wrote for the training data. - It normalizes each image per batch , meaning that it uses batch statistics. - We should not do this with the test and validation data, since in a real life scenario we don't process incoming images a batch at a time (we process one image at a time). - Knowing the average per batch of test data would effectively give our model an advantage. - The model should not have any information about the test data. What we need to do is normalize incoming test data using the statistics computed from the training set . * We implement this in the function below. * There is one technical note. Ideally, we would want to compute our sample mean and standard deviation using the entire training set. * However, since this is extremely large, that would be very time consuming. * In the interest of time, we'll take a random sample of the dataset and calcualte the sample mean and sample standard deviation. def get_test_and_valid_generator (valid_df, test_df, train_df, image_dir, x_col, y_cols, sample_size = 100 , batch_size = 8 , seed = 1 , target_w = 320 , target_h = 320 ): \"\"\" Return generator for validation set and test test set using normalization statistics from training set. Args: valid_df (dataframe): dataframe specifying validation data. test_df (dataframe): dataframe specifying test data. train_df (dataframe): dataframe specifying training data. image_dir (str): directory where image files are held. x_col (str): name of column in df that holds filenames. y_cols (list): list of strings that hold y labels for images. sample_size (int): size of sample to use for normalization statistics. batch_size (int): images per batch to be fed into model during training. seed (int): random seed. target_w (int): final width of input images. target_h (int): final height of input images. Returns: test_generator (DataFrameIterator) and valid_generator: iterators over test set and validation set respectively \"\"\" print ( \"getting train and valid generators...\" ) # get generator to sample dataset raw_train_generator = ImageDataGenerator() . flow_from_dataframe( dataframe = train_df, directory = IMAGE_DIR, x_col = \"Image\" , y_col = labels, class_mode = \"raw\" , batch_size = sample_size, shuffle = True , target_size = (target_w, target_h)) # get data sample batch = raw_train_generator . next() data_sample = batch[ 0 ] # use sample to fit mean and std for test set generator image_generator = ImageDataGenerator( featurewise_center = True , featurewise_std_normalization = True ) # fit generator to sample from training data image_generator . fit(data_sample) # get test generator valid_generator = image_generator . flow_from_dataframe( dataframe = valid_df, directory = image_dir, x_col = x_col, y_col = y_cols, class_mode = \"raw\" , batch_size = batch_size, shuffle = False , seed = seed, target_size = (target_w,target_h)) test_generator = image_generator . flow_from_dataframe( dataframe = test_df, directory = image_dir, x_col = x_col, y_col = y_cols, class_mode = \"raw\" , batch_size = batch_size, shuffle = False , seed = seed, target_size = (target_w,target_h)) return valid_generator, test_generator With our generator function ready, let's make one generator for our training data and one each of our test and validation datasets. IMAGE_DIR = data_dir + \"nih/images-small/\" train_generator = get_train_generator(train_df, IMAGE_DIR, \"Image\" , labels) valid_generator, test_generator = get_test_and_valid_generator(valid_df, test_df, train_df, IMAGE_DIR, \"Image\" , labels) getting train generator... Found 1000 validated image filenames. getting train and valid generators... Found 1000 validated image filenames. Found 1 validated image filenames. Found 420 validated image filenames. /usr/local/lib/python3.7/site-packages/keras_preprocessing/image/dataframe_iterator.py:273: UserWarning: Found 108 invalid image filename(s) in x_col=\"Image\". These filename(s) will be ignored. .format(n_invalid, x_col) Let's peek into what the generator gives our model during training and validation. We can do this by calling the __get_item__(index) function: x, y = train_generator . __getitem__ ( 0 ) #print (y) plt . imshow(x[ 0 ]); Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). print ( f\"x.shape= { x . shape } \\n y.shape= { y . shape } \" ) x.shape= (8, 320, 320, 3) y.shape= (8, 14)","title":"Build a separate generator for valid and test sets"},{"location":"ml/chest_xray/#3-model-development","text":"Now we'll move on to model training and development. We have a few practical challenges to deal with before actually training a neural network, though. The first is class imbalance.","title":"3 Model Development"},{"location":"ml/chest_xray/#31-addressing-class-imbalance","text":"One of the challenges with working with medical diagnostic datasets is the large class imbalance present in such datasets. Let's plot the frequency of each of the labels in our dataset: values = np . mean(train_generator . labels, axis = 0 ) sns . barplot(values, labels, order = labels) plt . yticks(fontsize = 13 ); plt . title( \"Frequency of Each Class\" , fontsize = 14 ); We can see from this plot that the prevalance of positive cases varies significantly across the different pathologies. (These trends mirror the ones in the full dataset as well.) * The Hernia pathology has the greatest imbalance with the proportion of positive training cases being about 0.2%. * But even the Infiltration pathology, which has the least amount of imbalance, has only 17.5% of the training cases labelled positive. Ideally, we would train our model using an evenly balanced dataset so that the positive and negative training cases would contribute equally to the loss. If we use a normal cross-entropy loss function with a highly unbalanced dataset, as we are seeing here, then the algorithm will be incentivized to prioritize the majority class (i.e negative in our case), since it contributes more to the loss.","title":"3.1 Addressing Class Imbalance"},{"location":"ml/chest_xray/#impact-of-class-imbalance-on-loss-function","text":"Let's take a closer look at this. Assume we would have used a normal cross-entropy loss for each pathology. We recall that the cross-entropy loss contribution from the $i^{th}$ training data case is: $$\\mathcal{L}_{cross-entropy}(x_i) = -(y_i \\log(f(x_i)) + (1-y_i) \\log(1-f(x_i))),$$ where $x_i$ and $y_i$ are the input features and the label, and $f(x_i)$ is the output of the model, i.e. the probability that it is positive. Note that for any training case, either $y_i=0$ or else $(1-y_i)=0$, so only one of these terms contributes to the loss (the other term is multiplied by zero, and becomes zero). We can rewrite the overall average cross-entropy loss over the entire training set $\\mathcal{D}$ of size $N$ as follows: $$\\mathcal{L} {cross-entropy}(\\mathcal{D}) = - \\frac{1}{N}\\big( \\sum {\\text{positive examples}} \\log (f(x_i)) + \\sum_{\\text{negative examples}} \\log(1-f(x_i)) \\big).$$ Using this formulation, we can see that if there is a large imbalance with very few positive training cases, for example, then the loss will be dominated by the negative class. Summing the contribution over all the training cases for each class (i.e. pathological condition), we see that the contribution of each class (i.e. positive or negative) is: $$freq_{p} = \\frac{\\text{number of positive examples}}{N} $$ $$\\text{and}$$ $$freq_{n} = \\frac{\\text{number of negative examples}}{N}.$$","title":"Impact of class imbalance on loss function"},{"location":"ml/chest_xray/#exercise-2-computing-class-frequencies","text":"Complete the function below to calculate these frequences for each label in our dataset. # UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) def compute_class_freqs (labels): \"\"\" Compute positive and negative frequences for each class. Args: labels (np.array): matrix of labels, size (num_examples, num_classes) Returns: positive_frequencies (np.array): array of positive frequences for each class, size (num_classes) negative_frequencies (np.array): array of negative frequences for each class, size (num_classes) \"\"\" N = labels . shape[ 0 ] positive_frequencies = np . sum(labels, axis = 0 ) / N negative_frequencies = 1. - positive_frequencies return positive_frequencies, negative_frequencies Now we'll compute frequencies for our training data. freq_pos, freq_neg = compute_class_freqs(train_generator . labels) freq_pos array([0.106, 0.02 , 0.033, 0.016, 0.128, 0.013, 0.014, 0.002, 0.175, 0.045, 0.054, 0.021, 0.01 , 0.038]) Let's visualize these two contribution ratios next to each other for each of the pathologies: data = pd . DataFrame( { 'Class' : labels, \"Positive_freq\" :freq_pos, \"Negative_freq\" :freq_neg } ) data . plot . bar(x = \"Class\" , y = [ \"Positive_freq\" , \"Negative_freq\" ], figsize = ( 15 , 4 ), color = [ 'blue' , 'orange' ]); plt . yticks(fontsize = 16 ); plt . xticks(fontsize = 16 ); plt . legend(fontsize = 16 ); The same bar plot can be plotted with seaborn in following way but remember sns barplot requires you to have a single row with multiple labels. For example: one column with Lables with values : positive and negative. and another column with values. Then we can plot the two labels with two barplots. data = pd . DataFrame({ \"Class\" : labels, \"Label\" : \"Positive\" , \"Value\" : freq_pos}) data = data . append([{ \"Class\" : labels[l], \"Label\" : \"Negative\" , \"Value\" : v} for l,v in enumerate (freq_neg)], ignore_index = True ) data . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } Class Label Value 0 Atelectasis Positive 0.106 1 Cardiomegaly Positive 0.020 2 Consolidation Positive 0.033 3 Edema Positive 0.016 4 Effusion Positive 0.128 plt . figure(figsize = ( 15 , 4 )) sns . barplot(x = \"Class\" , y = \"Value\" , hue = \"Label\" ,data = data); plt . xticks(rotation = 90 ); plt . yticks(fontsize = 16 ); plt . xticks(fontsize = 16 ); plt . legend(fontsize = 16 ); As we see in the above plot, the contributions of positive cases is significantly lower than that of the negative ones. However, we want the contributions to be equal. One way of doing this is by multiplying each example from each class by a class-specific weight factor, $w_{pos}$ and $w_{neg}$, so that the overall contribution of each class is the same. To have this, we want $$w_{pos} \\times freq_{p} = w_{neg} \\times freq_{n},$$ which we can do simply by taking $$w_{pos} = freq_{neg}$$ $$w_{neg} = freq_{pos}$$ This way, we will be balancing the contribution of positive and negative labels. pos_weights = freq_neg neg_weights = freq_pos pos_contribution = freq_pos * pos_weights neg_contribution = freq_neg * neg_weights Let's verify this by graphing the two contributions next to each other again: data = pd . DataFrame({ \"Class\" : labels, \"Label\" : \"Positive\" , \"Value\" : pos_contribution}) data = data . append([{ \"Class\" : labels[l], \"Label\" : \"Negative\" , \"Value\" : v} for l,v in enumerate (neg_contribution)], ignore_index = True ) plt . figure(figsize = ( 12 , 4 )) plt . xticks(rotation = 90 ) sns . barplot(x = \"Class\" , y = \"Value\" , hue = \"Label\" ,data = data); plt . yticks(fontsize = 16 ); plt . xticks(fontsize = 16 ); plt . legend(fontsize = 16 ); As the above figure shows, by applying these weightings the positive and negative labels within each class would have the same aggregate contribution to the loss function. Now let's implement such a loss function. After computing the weights, our final weighted loss for each training case will be $$ \\mathcal{L} {cross-entropy}^{w}(x) = - (w {p} y \\log(f(x)) + w_{n}(1-y) \\log( 1 - f(x) ) ) $$","title":"Exercise 2 - Computing Class Frequencies"},{"location":"ml/chest_xray/#exercise-3-weighted-loss","text":"Fill out the weighted_loss function below to return a loss function that calculates the weighted loss for each batch. Recall that for the multi-class loss, we add up the average loss for each individual class. Note that we also want to add a small value, $\\epsilon$, to the predicted values before taking their logs. This is simply to avoid a numerical error that would otherwise occur if the predicted value happens to be zero.","title":"Exercise 3 - Weighted Loss"},{"location":"ml/chest_xray/#note","text":"Please use Keras functions to calculate the mean and the log. Keras.mean Keras.log","title":"Note"},{"location":"ml/chest_xray/#note-on-converting-the-tf-1-code-to-tf-2","text":"First remove the sess = K.get_session(); with sess.as_default() as sess: line. Replace variable.eval() by variable.numpy() # UNQ_C3 (UNIQUE CELL IDENTIFIER, DO NOT EDIT) def get_weighted_loss (pos_weights, neg_weights, epsilon = 1e-7 ): \"\"\" Return weighted loss function given negative weights and positive weights. Args: pos_weights (np.array): array of positive weights for each class, size (num_classes) neg_weights (np.array): array of negative weights for each class, size (num_classes) Returns: weighted_loss (function): weighted loss function \"\"\" def weighted_loss (y_true, y_pred): \"\"\" Return weighted loss value. Args: y_true (Tensor): Tensor of true labels, size is (num_examples, num_classes) y_pred (Tensor): Tensor of predicted labels, size is (num_examples, num_classes) Returns: loss (Float): overall scalar loss summed across all classes \"\"\" # initialize loss to zero loss = 0.0 ### START CODE HERE (REPLACE INSTANCES OF 'None' with your code) ### for i in range ( len (pos_weights)): # for each class, add average weighted loss for that class loss += K . mean( - (pos_weights[i] * y_true[:,i] * K . log(y_pred[:,i] + epsilon) + neg_weights[i] * ( 1 - y_true[:,i]) * K . log( 1 - y_pred[:,i] + epsilon))) return loss return weighted_loss Now let's test our function with some simple cases.","title":"Note on converting the TF-1 code to TF-2"},{"location":"ml/chest_xray/#33-densenet121","text":"Next, we will use a pre-trained DenseNet121 model which we can load directly from Keras and then add two layers on top of it: 1. A GlobalAveragePooling2D layer to get the average of the last convolution layers from DenseNet121. 2. A Dense layer with sigmoid activation to get the prediction logits for each of our classes. We can set our custom loss function for the model by specifying the loss parameter in the compile() function. # create the base pre-trained model base_model = DenseNet121(weights = './nih/densenet.hdf5' , include_top = False ) x = base_model . output # add a global spatial average pooling layer x = GlobalAveragePooling2D()(x) # and a logistic layer predictions = Dense( len (labels), activation = \"sigmoid\" )(x) model = Model(inputs = base_model . input, outputs = predictions) model . compile(optimizer = 'adam' , loss = get_weighted_loss(pos_weights, neg_weights))","title":"3.3 DenseNet121"},{"location":"ml/chest_xray/#4-training-optional","text":"With our model ready for training, we will use the model.fit() function in Keras to train our model. - We are training on a small subset of the dataset (~1%). - So what we care about at this point is to make sure that the loss on the training set is decreasing. Since training can take a considerable time, for pedagogical purposes we have chosen not to train the model here but rather to load a set of pre-trained weights in the next section. However, you can use the code shown below to practice training the model locally on your machine or in Colab. NOTE: Do not run the code below on the Coursera platform as it will exceed the platform's memory limitations. Python Code for training the model: history = model . fit_generator(train_generator, validation_data = valid_generator, steps_per_epoch = 100 , validation_steps = 25 , epochs = 3 ) plt . plot(history . history[ 'loss' ]) plt . ylabel( \"loss\" ) plt . xlabel( \"epoch\" ) plt . title( \"Training Loss Curve\" ) plt . show()","title":"4 Training [optional]"},{"location":"ml/chest_xray/#41-training-on-the-larger-dataset","text":"Given that the original dataset is 40GB+ in size and the training process on the full dataset takes a few hours, we have trained the model on a GPU-equipped machine for you and provided the weights file from our model (with a batch size of 32 instead) to be used for the rest of this assignment. The model architecture for our pre-trained model is exactly the same, but we used a few useful Keras \"callbacks\" for this training. Do spend time to read about these callbacks at your leisure as they will be very useful for managing long-running training sessions: You can use ModelCheckpoint callback to monitor your model's val_loss metric and keep a snapshot of your model at the point. You can use the TensorBoard to use the Tensorflow Tensorboard utility to monitor your runs in real-time. You can use the ReduceLROnPlateau to slowly decay the learning rate for your model as it stops getting better on a metric such as val_loss to fine-tune the model in the final steps of training. You can use the EarlyStopping callback to stop the training job when your model stops getting better in it's validation loss. You can set a patience value which is the number of epochs the model does not improve after which the training is terminated. This callback can also conveniently restore the weights for the best metric at the end of training to your model. You can read about these callbacks and other useful Keras callbacks here . Let's load our pre-trained weights into the model now: model . load_weights( \"./nih/pretrained_model.h5\" )","title":"4.1 Training on the Larger Dataset"},{"location":"ml/chest_xray/#5-prediction-and-evaluation","text":"Now that we have a model, let's evaluate it using our test set. We can conveniently use the predict_generator function to generate the predictions for the images in our test set. Note: The following cell can take about 4 minutes to run. predicted_vals = model . predict_generator(test_generator, steps = len (test_generator))","title":"5 Prediction and Evaluation"},{"location":"ml/chest_xray/#51-roc-curve-and-auroc","text":"We'll cover topic of model evaluation in much more detail in later weeks, but for now we'll walk through computing a metric called the AUC (Area Under the Curve) from the ROC ( Receiver Operating Characteristic ) curve. This is also referred to as the AUROC value, but you will see all three terms in reference to the technique, and often used almost interchangeably. For now, what you need to know in order to interpret the plot is that a curve that is more to the left and the top has more \"area\" under it, and indicates that the model is performing better. We will use the util.get_roc_curve() function which has been provided for you in util.py . Look through this function and note the use of the sklearn library functions to generate the ROC curves and AUROC values for our model. roc_curve roc_auc_score auc_rocs = util . get_roc_curve(labels, predicted_vals, test_generator) You can compare the performance to the AUCs reported in the original ChexNeXt paper in the table below: For reference, here's the AUC figure from the ChexNeXt paper which includes AUC values for their model as well as radiologists on this dataset: This method does take advantage of a few other tricks such as self-training and ensembling as well, which can give a significant boost to the performance. For details about the best performing methods and their performance on this dataset, we encourage you to read the following papers: - CheXNet - CheXpert - ChexNeXt","title":"5.1 ROC Curve and AUROC"},{"location":"ml/chest_xray/#52-visualizing-learning-with-gradcam","text":"One of the challenges of using deep learning in medicine is that the complex architecture used for neural networks makes them much harder to interpret compared to traditional machine learning models (e.g. linear models). One of the most common approaches aimed at increasing the interpretability of models for computer vision tasks is to use Class Activation Maps (CAM). - Class activation maps are useful for understanding where the model is \"looking\" when classifying an image. In this section we will use a GradCAM's technique to produce a heatmap highlighting the important regions in the image for predicting the pathological condition. - This is done by extracting the gradients of each predicted class, flowing into our model's final convolutional layer. Look at the util.compute_gradcam which has been provided for you in util.py to see how this is done with the Keras framework. It is worth mentioning that GradCAM does not provide a full explanation of the reasoning for each classification probability. - However, it is still a useful tool for \"debugging\" our model and augmenting our prediction so that an expert could validate that a prediction is indeed due to the model focusing on the right regions of the image. First we will load the small training set and setup to look at the 4 classes with the highest performing AUC measures. df = pd . read_csv(data_dir + \"nih/train-small.csv\" ) IMAGE_DIR = data_dir + \"nih/images-small/\" # only show the labels with top 4 AUC labels_to_show = np . take(labels, np . argsort(auc_rocs)[:: - 1 ])[: 4 ] Now let's look at a few specific images. util . compute_gradcam(model, '00008270_015.png' , IMAGE_DIR, df, labels, labels_to_show) util . compute_gradcam(model, '00011355_002.png' , IMAGE_DIR, df, labels, labels_to_show) util . compute_gradcam(model, '00029855_001.png' , IMAGE_DIR, df, labels, labels_to_show) util . compute_gradcam(model, '00005410_000.png' , IMAGE_DIR, df, labels, labels_to_show) Congratulations, you've completed the first assignment of course one! You've learned how to preprocess data, check for data leakage, train a pre-trained model, and evaluate using the AUC. Great work!","title":"5.2 Visualizing Learning with GradCAM"},{"location":"ml/compare_classification/","text":"How to Evaluate Gradient Boosting Models with XGBoost in Python Source: https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/ Prepare the data ## Install xgboost as ##!pip3 install xgboost import xgboost print ( 'xgboost version:' , xgboost . __version__) xgboost version: 0.90 import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd . read_csv( './data/pima-indians-diabetes.data.csv' , header = None ) print ( 'shape(data):' ,data . shape) shape(data): (768, 9) data . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 # the last column is the yes/no column where 0 represents NO diabetes and 1 represent YES daibetes y = data . iloc[:, - 1 ] X = data . iloc[:,: - 1 ] X . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 0 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33 , random_state = 1728 ) print ( 'X_train.shape, X_test.shape, y_train.shape, y_test.shape' ) print (X_train . shape, X_test . shape, y_train . shape, y_test . shape) X_train.shape, X_test.shape, y_train.shape, y_test.shape (514, 8) (254, 8) (514,) (254,) Clasiffication using different Classifiers XGBOOST XGBoost article on towardsdatascience from xgboost import XGBClassifier xgb = XGBClassifier() xgb . fit(X_train, y_train) # 'eval_set' is another arguement for the validation data but not used here. y_pred = xgb . predict(X_test) preds = [ round (yp) for yp in y_pred] #accuracy can also be measured as #score = xgb.score(X_test, y_test) from sklearn.metrics import accuracy_score acc = accuracy_score(y_test, preds) print ( \"accuracy= %.2f \" % (acc * 100 )) accuracy=77.56 XGBOOST with With k-fold cross validation from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score kfold = KFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.38 from sklearn.model_selection import StratifiedKFold kfold = StratifiedKFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.66 If in doubt, use 10-fold cross validation for regression problems and stratified 10-fold cross validation on classification problems. Support Vector Machine from sklearn.svm import SVC clf_ = SVC(gamma = 2 , C = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"SVM Classifier, score : %.2f \" % (score * 100 )) SVM Classifier, score : 66.54 K-Nearest Neighbour from sklearn.neighbors import KNeighborsClassifier clf_ = KNeighborsClassifier( 3 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"K nearest neighbour Classifier, score : %.2f \" % (score * 100 )) K nearest neighbour Classifier, score : 71.65 Random Forrest from sklearn.ensemble import RandomForestClassifier clf_ = RandomForestClassifier(max_depth = 5 , n_estimators = 10 , max_features = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Random Forest Classifier, score : %.2f \" % (score * 100 )) Random Forest Classifier, score : 75.98 Decision Tree from sklearn.tree import DecisionTreeClassifier clf_ = DecisionTreeClassifier(max_depth = 5 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Decision Tree Classifier, score : %.2f \" % (score * 100 )) Decision Tree Classifier, score : 75.98 Adaboost I don't know much about it. from sklearn.ensemble import AdaBoostClassifier clf_ = AdaBoostClassifier() clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Adaboost Classifier, score : %.2f \" % (score * 100 )) Adaboost Classifier, score : 75.98","title":"Classification Comarision"},{"location":"ml/compare_classification/#how-to-evaluate-gradient-boosting-models-with-xgboost-in-python","text":"Source: https://machinelearningmastery.com/evaluate-gradient-boosting-models-xgboost-python/","title":"How to Evaluate Gradient Boosting Models with XGBoost in Python"},{"location":"ml/compare_classification/#prepare-the-data","text":"## Install xgboost as ##!pip3 install xgboost import xgboost print ( 'xgboost version:' , xgboost . __version__) xgboost version: 0.90 import numpy as np import pandas as pd import matplotlib.pyplot as plt data = pd . read_csv( './data/pima-indians-diabetes.data.csv' , header = None ) print ( 'shape(data):' ,data . shape) shape(data): (768, 9) data . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 8 0 6 148 72 35 0 33.6 0.627 50 1 1 1 85 66 29 0 26.6 0.351 31 0 # the last column is the yes/no column where 0 represents NO diabetes and 1 represent YES daibetes y = data . iloc[:, - 1 ] X = data . iloc[:,: - 1 ] X . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } 0 1 2 3 4 5 6 7 0 6 148 72 35 0 33.6 0.627 50 1 1 85 66 29 0 26.6 0.351 31 from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33 , random_state = 1728 ) print ( 'X_train.shape, X_test.shape, y_train.shape, y_test.shape' ) print (X_train . shape, X_test . shape, y_train . shape, y_test . shape) X_train.shape, X_test.shape, y_train.shape, y_test.shape (514, 8) (254, 8) (514,) (254,)","title":"Prepare the data"},{"location":"ml/compare_classification/#clasiffication-using-different-classifiers","text":"","title":"Clasiffication using different Classifiers"},{"location":"ml/compare_classification/#xgboost","text":"XGBoost article on towardsdatascience from xgboost import XGBClassifier xgb = XGBClassifier() xgb . fit(X_train, y_train) # 'eval_set' is another arguement for the validation data but not used here. y_pred = xgb . predict(X_test) preds = [ round (yp) for yp in y_pred] #accuracy can also be measured as #score = xgb.score(X_test, y_test) from sklearn.metrics import accuracy_score acc = accuracy_score(y_test, preds) print ( \"accuracy= %.2f \" % (acc * 100 )) accuracy=77.56","title":"XGBOOST"},{"location":"ml/compare_classification/#xgboost-with-with-k-fold-cross-validation","text":"from sklearn.model_selection import KFold from sklearn.model_selection import cross_val_score kfold = KFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.38 from sklearn.model_selection import StratifiedKFold kfold = StratifiedKFold(n_splits = 10 , random_state = 1728 , shuffle = True ) results = cross_val_score(xgb, X, y, cv = kfold) print ( \"accuracy: %.2f \" % (results . mean() * 100 )) accuracy: 75.66 If in doubt, use 10-fold cross validation for regression problems and stratified 10-fold cross validation on classification problems.","title":"XGBOOST with With k-fold cross validation"},{"location":"ml/compare_classification/#support-vector-machine","text":"from sklearn.svm import SVC clf_ = SVC(gamma = 2 , C = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"SVM Classifier, score : %.2f \" % (score * 100 )) SVM Classifier, score : 66.54","title":"Support Vector Machine"},{"location":"ml/compare_classification/#k-nearest-neighbour","text":"from sklearn.neighbors import KNeighborsClassifier clf_ = KNeighborsClassifier( 3 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"K nearest neighbour Classifier, score : %.2f \" % (score * 100 )) K nearest neighbour Classifier, score : 71.65","title":"K-Nearest Neighbour"},{"location":"ml/compare_classification/#random-forrest","text":"from sklearn.ensemble import RandomForestClassifier clf_ = RandomForestClassifier(max_depth = 5 , n_estimators = 10 , max_features = 1 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Random Forest Classifier, score : %.2f \" % (score * 100 )) Random Forest Classifier, score : 75.98","title":"Random Forrest"},{"location":"ml/compare_classification/#decision-tree","text":"from sklearn.tree import DecisionTreeClassifier clf_ = DecisionTreeClassifier(max_depth = 5 ) clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Decision Tree Classifier, score : %.2f \" % (score * 100 )) Decision Tree Classifier, score : 75.98","title":"Decision Tree"},{"location":"ml/compare_classification/#adaboost","text":"I don't know much about it. from sklearn.ensemble import AdaBoostClassifier clf_ = AdaBoostClassifier() clf_ . fit(X_train, y_train) score = clf_ . score(X_test, y_test) print ( \"Adaboost Classifier, score : %.2f \" % (score * 100 )) Adaboost Classifier, score : 75.98","title":"Adaboost"},{"location":"ml/deepnn_imp/","text":"Implimenting the Deep Neural Network from Scratch Bring your code here. Deep Learning-I (C) Basics: Initialization, Regularization, Gradient Checking (C) Optimization (C) Foundations of CNN (C) Residual Networks and Keras Tutorial (C) Car Detection and Autonomous driving: (C)","title":"Implimenting Deep Neural Network"},{"location":"ml/deepnn_imp/#implimenting-the-deep-neural-network-from-scratch","text":"Bring your code here. Deep Learning-I (C) Basics: Initialization, Regularization, Gradient Checking (C) Optimization (C) Foundations of CNN (C) Residual Networks and Keras Tutorial (C) Car Detection and Autonomous driving: (C)","title":"Implimenting the Deep Neural Network from Scratch"},{"location":"ml/file0/","text":"Welcome to MkDocs For full documentation you can visit mkdocs.org . Commands ls -la : is the command for listing files Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Welcome to MkDocs"},{"location":"ml/file0/#welcome-to-mkdocs","text":"For full documentation you can visit mkdocs.org .","title":"Welcome to MkDocs"},{"location":"ml/file0/#commands","text":"ls -la : is the command for listing files","title":"Commands"},{"location":"ml/file0/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"ml/learn_ml/","text":"Machine Learning Learning Regression Simple Linear Regression (SLR) (CE) Multiple Linear Regression (MLR) (E) Interaction terms, polynomials and other transforms (E) Overfitting, The Bias-Variance Trade-off and Multicollinearity (E) Regularization with LASSO and Ridge Regression (CE) Hands On ML, Geron Some notebooks Classification Logistic Regression: The Algorithm (E) Logistic Regression: Build a classifier (E) Decision Trees (E) Random Forests (E) Categorical Data in Tree Based algorithm (E) Support Vector Machine (SVM) (CE) Ensamble Methods (E) Train Test Splits (E) Oversampling and Undersampling (E) Hands ON ML (Geron) Unsupervised Learning Clustering (K-means) : Unsupervised Learning (C) Clustering Practices (E) Dimensionality Reduction : PCA (CE) Dimensionality Reduction: Manifold Learning (E) Dimensionality Reduction: Practices (E) Proper Data Preprocessing Steps (E) Time Series Analysis (E) Introduction to Time Series Data Lag and Autocorrelation Time Series Test Errors and CV Exponential Smoothing ARIMA Model Neural Network (I) (E) Perceptrons (E) Multilayer Networks (E) Introduction to Neural Network (C) Deep Learning-I (C) Logistic Regression (C) Shallow Neural Networks (C) Building Deep Neural Network (C) Deep Learning - II (C) Basics: Initialization, Regularization, Gradient Checking (C) Optimization (C) Introduction to Tensor flow (C) Deep Learning - IV (C) Foundations of CNN (C) Residual Networks and Keras Tutorial (C) Car Detection and Autonomous driving: (C) Face recognition and art generation using NN (C) Natural Language Processing Basics from Geron","title":"Machine Learning"},{"location":"ml/learn_ml/#machine-learning","text":"","title":"Machine Learning"},{"location":"ml/learn_ml/#learning","text":"","title":"Learning"},{"location":"ml/learn_ml/#regression","text":"Simple Linear Regression (SLR) (CE) Multiple Linear Regression (MLR) (E) Interaction terms, polynomials and other transforms (E) Overfitting, The Bias-Variance Trade-off and Multicollinearity (E) Regularization with LASSO and Ridge Regression (CE) Hands On ML, Geron Some notebooks","title":"Regression"},{"location":"ml/learn_ml/#classification","text":"Logistic Regression: The Algorithm (E) Logistic Regression: Build a classifier (E) Decision Trees (E) Random Forests (E) Categorical Data in Tree Based algorithm (E) Support Vector Machine (SVM) (CE) Ensamble Methods (E) Train Test Splits (E) Oversampling and Undersampling (E) Hands ON ML (Geron)","title":"Classification"},{"location":"ml/learn_ml/#unsupervised-learning","text":"Clustering (K-means) : Unsupervised Learning (C) Clustering Practices (E) Dimensionality Reduction : PCA (CE) Dimensionality Reduction: Manifold Learning (E) Dimensionality Reduction: Practices (E) Proper Data Preprocessing Steps (E)","title":"Unsupervised Learning"},{"location":"ml/learn_ml/#time-series-analysis-e","text":"Introduction to Time Series Data Lag and Autocorrelation Time Series Test Errors and CV Exponential Smoothing ARIMA Model","title":"Time Series Analysis (E)"},{"location":"ml/learn_ml/#neural-network-i-e","text":"Perceptrons (E) Multilayer Networks (E) Introduction to Neural Network (C) Deep Learning-I (C) Logistic Regression (C) Shallow Neural Networks (C) Building Deep Neural Network (C) Deep Learning - II (C) Basics: Initialization, Regularization, Gradient Checking (C) Optimization (C) Introduction to Tensor flow (C) Deep Learning - IV (C) Foundations of CNN (C) Residual Networks and Keras Tutorial (C) Car Detection and Autonomous driving: (C) Face recognition and art generation using NN (C)","title":"Neural Network (I) (E)"},{"location":"ml/learn_ml/#natural-language-processing","text":"Basics from Geron","title":"Natural Language Processing"},{"location":"ml/ml_basics/","text":"Machine Learning Basics Linear and Logistic Regression Decision Trees (E), Random Forests (E) and Support Vector Machine (SVM) (CE) Clustering (K-means) : Unsupervised Learning (C) Dimensionality Reduction : PCA (CE)","title":"Implimenting the Basics"},{"location":"ml/ml_basics/#machine-learning-basics","text":"Linear and Logistic Regression Decision Trees (E), Random Forests (E) and Support Vector Machine (SVM) (CE) Clustering (K-means) : Unsupervised Learning (C) Dimensionality Reduction : PCA (CE)","title":"Machine Learning Basics"},{"location":"ml/ml_content/","text":"Machine Learning Learning Regression Linear Regression (C) Logistic Regression (C) Regularization (C) Simple Linear Regression (SLR) E Multiple Linear Regression (MLR) (E) Interaction terms, polynomials and other transforms (E) Overfitting, The Bias-Variance Trade-off and Multicollinearity (E) Regularization with LASSO and Ridge Regression (E) Hands On ML, Geron Some notebooks Classification Logistic Regression: The Algorithm (E) Logistic Regression: Build a classifier (E) Decision Trees (E) Random Forests (E) Categorical Data in Tree Based algorithm (E) Support Vector Machine (SVM) (C) Support Vector Machine (SVM) (E) Ensamble Methods (E) Train Test Splits (E) Oversampling and Undersampling (E) Hands ON ML (Geron) Unsupervised Learning Clustering (K-means) : Unsupervised Learning (C) ClusteringClustering Practices (E) Dimensionality Reduction : PCA (C) Dimensionality Reduction: PCA (E) Dimensionality Reduction: Manifold Learning (E) Dimensionality Reduction: Practices (E) Proper Data Preprocessing Steps (E) Time Series Analysis (E) Introduction to Time Series Data Lag and Autocorrelation Time Series Test Errors and CV Exponential Smoothing ARIMA Model Neural Network (I) (E) Perceptrons (E) Multilayer Networks (E) Introduction to Neural Network (C) Deep Learning-I (C) Logistic Regression (C) Shallow Neural Networks (C) Building Deep Neural Network (C) Deep Learning - II (C) Basics: Initialization, Regularization, Gradient Checking (C) Optimization (C) Introduction to Tensor flow (C) Deep Learning - IV (C) Foundations of CNN (C) Residual Networks and Keras Tutorial (C) Car Detection and Autonomous driving: (C) Face recognition and art generation using NN (C) Natural Language Processing Basics from Geron","title":"Machine Learning"},{"location":"ml/ml_content/#machine-learning","text":"","title":"Machine Learning"},{"location":"ml/ml_content/#learning","text":"","title":"Learning"},{"location":"ml/ml_content/#regression","text":"Linear Regression (C) Logistic Regression (C) Regularization (C) Simple Linear Regression (SLR) E Multiple Linear Regression (MLR) (E) Interaction terms, polynomials and other transforms (E) Overfitting, The Bias-Variance Trade-off and Multicollinearity (E) Regularization with LASSO and Ridge Regression (E) Hands On ML, Geron Some notebooks","title":"Regression"},{"location":"ml/ml_content/#classification","text":"Logistic Regression: The Algorithm (E) Logistic Regression: Build a classifier (E) Decision Trees (E) Random Forests (E) Categorical Data in Tree Based algorithm (E) Support Vector Machine (SVM) (C) Support Vector Machine (SVM) (E) Ensamble Methods (E) Train Test Splits (E) Oversampling and Undersampling (E) Hands ON ML (Geron)","title":"Classification"},{"location":"ml/ml_content/#unsupervised-learning","text":"Clustering (K-means) : Unsupervised Learning (C) ClusteringClustering Practices (E) Dimensionality Reduction : PCA (C) Dimensionality Reduction: PCA (E) Dimensionality Reduction: Manifold Learning (E) Dimensionality Reduction: Practices (E) Proper Data Preprocessing Steps (E)","title":"Unsupervised Learning"},{"location":"ml/ml_content/#time-series-analysis-e","text":"Introduction to Time Series Data Lag and Autocorrelation Time Series Test Errors and CV Exponential Smoothing ARIMA Model","title":"Time Series Analysis (E)"},{"location":"ml/ml_content/#neural-network-i-e","text":"Perceptrons (E) Multilayer Networks (E) Introduction to Neural Network (C) Deep Learning-I (C) Logistic Regression (C) Shallow Neural Networks (C) Building Deep Neural Network (C) Deep Learning - II (C) Basics: Initialization, Regularization, Gradient Checking (C) Optimization (C) Introduction to Tensor flow (C) Deep Learning - IV (C) Foundations of CNN (C) Residual Networks and Keras Tutorial (C) Car Detection and Autonomous driving: (C) Face recognition and art generation using NN (C)","title":"Neural Network (I) (E)"},{"location":"ml/ml_content/#natural-language-processing","text":"Basics from Geron","title":"Natural Language Processing"},{"location":"ml/projects/","text":"Machine Learning Projects Time Series Analysis of Temperature and Polution data project2","title":"Machine Learning Projects"},{"location":"ml/projects/#machine-learning-projects","text":"","title":"Machine Learning Projects"},{"location":"ml/projects/#time-series-analysis-of-temperature-and-polution-data","text":"","title":"Time Series Analysis of Temperature and Polution data"},{"location":"ml/projects/#project2","text":"","title":"project2"},{"location":"ml/tf/","text":"Introducion to TensorFlow-2 Bring intro from Geron book as well coursera course.","title":"Introducion to TensorFlow-2"},{"location":"ml/tf/#introducion-to-tensorflow-2","text":"Bring intro from Geron book as well coursera course.","title":"Introducion to TensorFlow-2"},{"location":"python/python_numpy_scipy/","text":"More on Numerics using Numpy/Scipy Derivative, Integration Root finding Random Walk Project Calculating value of $\\pi$ Curve fitting [Next class] 1. Derivative $$f(x) = x^2 + 5x -4 $$ $$\\frac{df(x)}{dx} = 2x+5$$ $$\\frac{df(x)}{dx} \\Big \\vert_{x=1} = 7$$ import numpy as np import pylab as plt from scipy.misc import derivative def f (x): return x ** 2 + 5 * x - 4 df = derivative(f, x0 = 1. ) print (df) 7.0 # SOLUTION HW1 from scipy.misc import derivative def f (x, a): return np . exp( - a * x ** 2 ) a = 1. ; x0 = 1 df = derivative(f, x0 = x0, dx = 1e-3 , args = (a,)) print (df) print ( - 2 * a * x0 * f(x0, a)) -0.7357586370898284 -0.7357588823428847 HW1 Calculate derivative $(d/dx)$ of a function : $f(x) = e^{-ax^2}$ at $x = 1 $ analytically. And Confirm your results with numerical implementation for any arbitrary value of $a$. Hint: use arguement $args=(a,)$ in the derivative call and give $a$ as second input arguement in the function. Hint: If your numerical answer doesn't match, use another arguement in the derivative call $dx=1e-3$ or some small number. 2. Integration 1. Integrate a Function $$I = \\int_0^1 (x^2+ 3x-1) \\ dx$$ $$I= \\frac{x^3}{3}+\\frac{3x^2}{2}-1x \\ \\ \\Big \\vert_0^1$$ $$ I = \\frac{1}{3}+\\frac{3}{2}-1 $$ $$ I = 0.833$$ from scipy.integrate import quad def func (x): return x ** 2 + 3 * x - 1 I, err = quad(func, 0 , 1 ) print (I, err) 0.8333333333333334 1.2723808807253818e-14 # SOLUTION HW2 from scipy.integrate import quad def func (x, a): return x ** 2 * np . exp( - a * x ** 2 ) a = 6 ; I, err = quad(func, 0 , np . inf, args = (a,)) # Analytic Answer I_ana = np . sqrt(np . pi) / ( 4. * a ** ( 1.5 )) print ( 'Analytic Answer:' , I_ana, 'Numerical Answer:' , I) Analytic Answer: 0.03015005227326115 Numerical Answer: 0.030150052273261157 HW2 Evaluate the following integration: $$I = \\int_0^{\\infty} x^2 e^{-ax^2} \\ dx$$ using both numerical and analytical methods. For any arbitrary value of $a$. Make sure they match with each other. Hint: use arguement $args=(a,)$ in the quad call and give $a$ as second input arguement in the function. Hint: Use Feynman's Derivative inside integral technique for the analytical evaluation. 2. Integrate a data set Trapezoidal Rule: https://en.wikipedia.org/wiki/Trapezoidal_rule Simpson Rule: https://en.wikipedia.org/wiki/Simpson%27s_rule # load the dataset data = np . loadtxt( 'data.dat' ) . T #print (data) #print (data.shape) #print (data[0]) x = data[ 0 ] y = data[ 1 ] plt . plot(x, y, '-o' ) [<matplotlib.lines.Line2D at 0x11d3e96d0>] from scipy.integrate import trapz, simps, romb Int = trapz(y, x) print (Int) Int = simps(y, x) print (Int) 94.0112 93.93663963963965 def func (x, a,b): return a * x ** 3 + b * np . sin(x) HW3 Implement an algebraic function $y$ of some input value $x$ print the output $y$ for different values of $x$. Save the data as some file. Load the file and do numerical integration using trapz as well as simps methods. Also calculate the integration of the function you used earlier analytically and compare your results. 3. Root Finding The root of a function is a point where f(x) = 0. We use scipy.optimize.root() from scipy import optimize import seaborn as sns sns . set() def f (x): return 0.1 * x ** 2 + 1. * np . sin(x) - 2. xx = np . linspace( - 10 , 10 , 50 ) yy = f(xx) plt . plot(xx, yy) #plt.grid() roots = optimize . root(f, x0 = [ - 5 , 5. ]) ## x0: initial guess print (roots . x) plt . axvline(x = roots . x[ 0 ], color = 'k' ) plt . axvline(x = roots . x[ 1 ], color = 'k' ) [-3.76415182 5.31434703] <matplotlib.lines.Line2D at 0x11fb95850> import matplotlib.pyplot as plt from scipy import optimize def f (x): return (x ** 3 - 2 * x ** 2 + 4 * x - 4. ) (a, b) = [ - 1.5 , 4 ] xx = np . linspace(a, b, 20 ) plt . plot(xx, f(xx)) root = optimize . brentq(f, a, b) plt . plot([root], [ 0. ], 'o' , color = 'maroon' , lw = 4. ) print (root) 1.2955977425220846 4. Finding Minimum of a function $$f(x) = x^2+10 \\ sin(x)$$ $$ \\frac{df(x)}{dx} = 2x+ 10 \\ cos(x) $$ Minimum occurs at $\\frac{df(x)}{dx} \\big \\vert_{x=x0} = 0$ def func (x): return 0.1 * x ** 2 + 1. * np . sin(x) + 1. xx = np . linspace( - 10 , 10 , 50 ) yy = func(xx) plt . plot(xx, yy) from scipy import optimize xmin = optimize . minimize(func, x0 = 0 ) print (xmin) plt . axvline(x = xmin . x, color = 'k' ) fun: 0.20541766243848236 hess_inv: array([[0.85898602]]) jac: array([-1.56462193e-07]) message: 'Optimization terminated successfully.' nfev: 18 nit: 5 njev: 6 status: 0 success: True x: array([-1.30644014]) <matplotlib.lines.Line2D at 0x11fcb1160> from scipy.interpolate import UnivariateSpline data = np . loadtxt( 'data.dat' ) . transpose() (x, y) = data plt . scatter(x, y, color = 'blue' ) func = UnivariateSpline(x, y, s = 0 ) print (func) #plt.scatter(10, func(10)) xx = np . linspace( 0 , 9 , 50 ) yy = func(xx) plt . plot(xx, yy, color = 'red' ) <scipy.interpolate.fitpack2.InterpolatedUnivariateSpline object at 0x11fd4d9d0> [<matplotlib.lines.Line2D at 0x11fd8c6d0>] y_new = y + 10 * np . random . rand( len (y)) plt . plot(x, y_new, 'o' , lw = 5. ) func = UnivariateSpline(x, y_new, s = 0 ) xx = np . linspace( min (x), max (x), 50 ) yy = func(xx) plt . plot(xx, yy, color = 'red' ) [<matplotlib.lines.Line2D at 0x11fe4cdf0>] 5. Random Numbers 1. Random Walk in 1D counter = [ 0 ] steps = 10000 for i in range (steps): r = 2 * np . random . rand() - 1. next_num = counter[ - 1 ] + r counter . append(next_num) plt . plot(counter, 'o' , markersize = 0.4 , color = 'maroon' ) [<matplotlib.lines.Line2D at 0x11fee73d0>] HW4 Perform the same 1D random walk using an array with zero values and populating the array at each random walk steps. 2. Random Walk in 2D # Random Walk in 2D steps = 100 counter = [ [ 0 , 0 ] ] for i in range (steps): rx = 2 * np . random . rand() - 1 ry = 2 * np . random . rand() - 1 last_point = counter[ - 1 ] new_point = np . array(last_point) + np . array([rx, ry]) counter . append(new_point) counter = np . array(counter) plt . plot(counter[:, 0 ], counter[:, 1 ], '-o' , lw = 0.5 , color = 'maroon' ) plt . scatter([ 0 ],[ 0 ], marker = 'o' , lw = 10 , color = 'blue' ) plt . scatter(counter[ - 1 ][ 0 ],counter[ - 1 ][ 1 ], marker = '*' , lw = 10 , color = 'green' ) plt . show() HW5 Perform the Similar 2D random walk with the counter being initialized as a zero Matrix (HINT: use np.zeros()) and the random number for each step in the range [-2, 2]. HINT: Change the scale of the random number which originally is in the range [0,1) to your range. Compute the average values of x coordinates as well as the y coordinates of all the points visited. HINT: use np.mean() Calculate the distance between first point and the last point visited. # HINT: Use standard mathematical formula Calculate the longest and the shortest distances of all the points (except the origin) from starting point., HINT: use np.max() and np.min() 6. Calculating value of $\\pi$ using random numbers Imagine a circle of radius r inscribed in a square of length 2*r. Through random number on the square. Check if it hits the circle or not. Count the points hitting the circle. For large N (number of dots) the total number of dots represent the area of the square. and the total dots inside the circle represents the are of the circle. Take ratio of area of the two. $$\\frac{A_{circle}}{A_{Square}} = \\frac{ \\pi r^2} { (2r)^2} = \\pi/4$$ $$\\pi = 4 \\frac{A_{circle}}{A_{Square}}$$ $$\\pi = 4 \\frac{N_{circle}}{N_{Square}}$$ # np.random.rand() gives values [0, 1) Maxiter = 100 N_circle = 0 N_square = 0 plt . figure(figsize = ( 6 , 6 )) for i in range (Maxiter): rx = 2 * np . random . rand() - 1 ry = 2 * np . random . rand() - 1 l = np . sqrt(rx ** 2 + ry ** 2 ) plt . scatter(rx, ry, color = 'blue' , lw = '.5' ) if l <= 1. : N_circle = N_circle + 1 plt . scatter(rx, ry, color = 'red' , lw = '.5' ) N_square += 1 plt . xlim([ - 1.2 , 1.2 ]) plt . ylim([ - 1.2 , 1.2 ]) plt . axhline(y = 0. , color = 'maroon' , lw = 2. ) plt . axvline(x = 0. , color = 'maroon' , lw = 2. ) print (N_circle, N_square) pi = 4 * N_circle / N_square circle = np . zeros(( 50 , 2 ), dtype = float ) theta = np . linspace( 0 , 2 * np . pi, 50 ) xt = np . cos(theta) yt = np . sin(theta) plt . plot(xt, yt, color = 'maroon' ) err = ( abs (pi - np . pi) / np . pi) * 100. print ( 'calculated pi=' ,pi, 'Exact pi=' , np . pi, 'Error=' , err, '%' ) 82 100 calculated pi= 3.28 Exact pi= 3.141592653589793 Error= 4.405642668283338 % HW6 In the above calculation of $\\pi$, draw a figure with circle inscribed in a square with all of your random dots. Use different colors for the dots inside and outside the circle. Plot vertical and horizontal line passing through the origin for the axis. Hints * You can use plt.axvline and plt.axhline for vertical and horizontal lines. and these can be used to draw the outer lines of the square as well. Just change the location of the lines. * Plotting a circle can be done in many ways, one way is to first generate points on the circle using $(r cos(\\theta), r sin(\\theta) )$ and taking $r=1$ and $\\theta \\in [0, 2\\pi]$ * Plotting dots can be done by doing a scatter plot at each step. Plot the inside dots inside your if (l<=1) statement. * You can use plt.figure(figsize=(8,8)) to make the aspect ratio same for both the axis. * First use only Mandelbrot set [Optional] Here is Wikipedia page on Mandelbrot Set. The Mandelbrot set is the set of values of c in the complex plane for which the orbit of 0 under iteration of the quadratic map $$z_{n+1}=z_{n}^{2}+c$$ remains bounded. A point $c$ belongs to the Mandelbrot set if and only if $ \\vert P_{c}^{n}(0) \\vert \\leq 2 $ for all $n \\ge 0 $ import numpy as np import pylab as plt import seaborn as sns #sns.set(False) def CheckBoundedness (c, Maxiter): z = 0 j for i in range (Maxiter): z = z ** ( 2 ) + c if abs (z) > 2 : return i return Maxiter N = 100 ; Maxiter = 80 ; xmin, xmax = [ - 2 , 1 ] ymin, ymax = [ - 2 , 2 ] xL = np . linspace(xmin, xmax, N) yL = np . linspace(ymin, ymax, N) data = np . zeros( (N, N), dtype = float ) for i in range (N): for j in range (N): c = xL[i] + 1 j * yL[j] data[i,j] = CheckBoundedness(c, Maxiter) plt . figure(figsize = ( 6 , 6 )); plt . imshow(data[:: - 1 ] . T, cmap = 'plasma' , extent = [xmin, xmax, ymin, ymax]); #'Spectral' is good #plt.xticks([]); #plt.yticks([]); Simple Pendulum $$ \\frac{d^2 \\theta}{dt^2} + a \\theta + b \\frac{d \\theta}{dt} = 0 $$ let $y = [ \\theta, \\omega ] $ where $\\omega = \\frac{d\\theta}{dt}$ Hence $$ \\frac{dy}{dt} = \\frac{d}{dt} [\\theta, \\omega] = \\frac{d}{dt} [\\theta, \\frac{d\\theta}{dt}] = [ \\frac{d \\theta}{dt}, \\frac{d^2 \\theta}{dt}] = [ \\frac{d \\theta}{dt} , -a \\theta - b \\frac{d \\theta}{dt} ] = [\\omega, -a \\theta - b \\omega] $$ import numpy as np import pylab as plt def pendulum (y,t, a, b): # y = [\\theta, \\omega] # we return [d \\theta /dt, -a \\theta - b \\omega] dydt = [ y[ 1 ], - a * y[ 0 ] - b * y[ 1 ] ] #dydt= [ y[1], -a*y[0] ] return np . array(dydt) y0 = [ 5. , 0. ] a = 1. b = 0.1 t = np . linspace( 0 , 80 , 500 ) from scipy.integrate import odeint sol = odeint(pendulum, y0, t, args = (a,b)) plt . figure( 1 ) #plt.subplot(121) plt . plot(t, sol[:, 0 ], label = '$ \\\\ theta(t)$' ); plt . plot(t, sol[:, 1 ], label = '$\\omega(t)$' ); plt . legend(frameon = False , fontsize = 16 ); plt . axhline(y = 0 , color = 'k' , ls = ':' );","title":"Numpy/Scipy examples"},{"location":"python/python_numpy_scipy/#more-on-numerics-using-numpyscipy","text":"Derivative, Integration Root finding Random Walk","title":"More on Numerics using Numpy/Scipy"},{"location":"python/python_numpy_scipy/#project","text":"Calculating value of $\\pi$ Curve fitting [Next class]","title":"Project"},{"location":"python/python_numpy_scipy/#1-derivative","text":"$$f(x) = x^2 + 5x -4 $$ $$\\frac{df(x)}{dx} = 2x+5$$ $$\\frac{df(x)}{dx} \\Big \\vert_{x=1} = 7$$ import numpy as np import pylab as plt from scipy.misc import derivative def f (x): return x ** 2 + 5 * x - 4 df = derivative(f, x0 = 1. ) print (df) 7.0 # SOLUTION HW1 from scipy.misc import derivative def f (x, a): return np . exp( - a * x ** 2 ) a = 1. ; x0 = 1 df = derivative(f, x0 = x0, dx = 1e-3 , args = (a,)) print (df) print ( - 2 * a * x0 * f(x0, a)) -0.7357586370898284 -0.7357588823428847","title":"1. Derivative"},{"location":"python/python_numpy_scipy/#hw1","text":"Calculate derivative $(d/dx)$ of a function : $f(x) = e^{-ax^2}$ at $x = 1 $ analytically. And Confirm your results with numerical implementation for any arbitrary value of $a$. Hint: use arguement $args=(a,)$ in the derivative call and give $a$ as second input arguement in the function. Hint: If your numerical answer doesn't match, use another arguement in the derivative call $dx=1e-3$ or some small number.","title":"HW1"},{"location":"python/python_numpy_scipy/#2-integration","text":"","title":"2. Integration"},{"location":"python/python_numpy_scipy/#1-integrate-a-function","text":"$$I = \\int_0^1 (x^2+ 3x-1) \\ dx$$ $$I= \\frac{x^3}{3}+\\frac{3x^2}{2}-1x \\ \\ \\Big \\vert_0^1$$ $$ I = \\frac{1}{3}+\\frac{3}{2}-1 $$ $$ I = 0.833$$ from scipy.integrate import quad def func (x): return x ** 2 + 3 * x - 1 I, err = quad(func, 0 , 1 ) print (I, err) 0.8333333333333334 1.2723808807253818e-14 # SOLUTION HW2 from scipy.integrate import quad def func (x, a): return x ** 2 * np . exp( - a * x ** 2 ) a = 6 ; I, err = quad(func, 0 , np . inf, args = (a,)) # Analytic Answer I_ana = np . sqrt(np . pi) / ( 4. * a ** ( 1.5 )) print ( 'Analytic Answer:' , I_ana, 'Numerical Answer:' , I) Analytic Answer: 0.03015005227326115 Numerical Answer: 0.030150052273261157","title":"1. Integrate a Function"},{"location":"python/python_numpy_scipy/#hw2","text":"Evaluate the following integration: $$I = \\int_0^{\\infty} x^2 e^{-ax^2} \\ dx$$ using both numerical and analytical methods. For any arbitrary value of $a$. Make sure they match with each other. Hint: use arguement $args=(a,)$ in the quad call and give $a$ as second input arguement in the function. Hint: Use Feynman's Derivative inside integral technique for the analytical evaluation.","title":"HW2"},{"location":"python/python_numpy_scipy/#2-integrate-a-data-set","text":"Trapezoidal Rule: https://en.wikipedia.org/wiki/Trapezoidal_rule Simpson Rule: https://en.wikipedia.org/wiki/Simpson%27s_rule # load the dataset data = np . loadtxt( 'data.dat' ) . T #print (data) #print (data.shape) #print (data[0]) x = data[ 0 ] y = data[ 1 ] plt . plot(x, y, '-o' ) [<matplotlib.lines.Line2D at 0x11d3e96d0>] from scipy.integrate import trapz, simps, romb Int = trapz(y, x) print (Int) Int = simps(y, x) print (Int) 94.0112 93.93663963963965 def func (x, a,b): return a * x ** 3 + b * np . sin(x)","title":"2. Integrate a data set"},{"location":"python/python_numpy_scipy/#hw3","text":"Implement an algebraic function $y$ of some input value $x$ print the output $y$ for different values of $x$. Save the data as some file. Load the file and do numerical integration using trapz as well as simps methods. Also calculate the integration of the function you used earlier analytically and compare your results.","title":"HW3"},{"location":"python/python_numpy_scipy/#3-root-finding","text":"The root of a function is a point where f(x) = 0. We use scipy.optimize.root() from scipy import optimize import seaborn as sns sns . set() def f (x): return 0.1 * x ** 2 + 1. * np . sin(x) - 2. xx = np . linspace( - 10 , 10 , 50 ) yy = f(xx) plt . plot(xx, yy) #plt.grid() roots = optimize . root(f, x0 = [ - 5 , 5. ]) ## x0: initial guess print (roots . x) plt . axvline(x = roots . x[ 0 ], color = 'k' ) plt . axvline(x = roots . x[ 1 ], color = 'k' ) [-3.76415182 5.31434703] <matplotlib.lines.Line2D at 0x11fb95850> import matplotlib.pyplot as plt from scipy import optimize def f (x): return (x ** 3 - 2 * x ** 2 + 4 * x - 4. ) (a, b) = [ - 1.5 , 4 ] xx = np . linspace(a, b, 20 ) plt . plot(xx, f(xx)) root = optimize . brentq(f, a, b) plt . plot([root], [ 0. ], 'o' , color = 'maroon' , lw = 4. ) print (root) 1.2955977425220846","title":"3. Root Finding"},{"location":"python/python_numpy_scipy/#4-finding-minimum-of-a-function","text":"$$f(x) = x^2+10 \\ sin(x)$$ $$ \\frac{df(x)}{dx} = 2x+ 10 \\ cos(x) $$ Minimum occurs at $\\frac{df(x)}{dx} \\big \\vert_{x=x0} = 0$ def func (x): return 0.1 * x ** 2 + 1. * np . sin(x) + 1. xx = np . linspace( - 10 , 10 , 50 ) yy = func(xx) plt . plot(xx, yy) from scipy import optimize xmin = optimize . minimize(func, x0 = 0 ) print (xmin) plt . axvline(x = xmin . x, color = 'k' ) fun: 0.20541766243848236 hess_inv: array([[0.85898602]]) jac: array([-1.56462193e-07]) message: 'Optimization terminated successfully.' nfev: 18 nit: 5 njev: 6 status: 0 success: True x: array([-1.30644014]) <matplotlib.lines.Line2D at 0x11fcb1160> from scipy.interpolate import UnivariateSpline data = np . loadtxt( 'data.dat' ) . transpose() (x, y) = data plt . scatter(x, y, color = 'blue' ) func = UnivariateSpline(x, y, s = 0 ) print (func) #plt.scatter(10, func(10)) xx = np . linspace( 0 , 9 , 50 ) yy = func(xx) plt . plot(xx, yy, color = 'red' ) <scipy.interpolate.fitpack2.InterpolatedUnivariateSpline object at 0x11fd4d9d0> [<matplotlib.lines.Line2D at 0x11fd8c6d0>] y_new = y + 10 * np . random . rand( len (y)) plt . plot(x, y_new, 'o' , lw = 5. ) func = UnivariateSpline(x, y_new, s = 0 ) xx = np . linspace( min (x), max (x), 50 ) yy = func(xx) plt . plot(xx, yy, color = 'red' ) [<matplotlib.lines.Line2D at 0x11fe4cdf0>]","title":"4. Finding Minimum of a function"},{"location":"python/python_numpy_scipy/#5-random-numbers","text":"","title":"5. Random Numbers"},{"location":"python/python_numpy_scipy/#1-random-walk-in-1d","text":"counter = [ 0 ] steps = 10000 for i in range (steps): r = 2 * np . random . rand() - 1. next_num = counter[ - 1 ] + r counter . append(next_num) plt . plot(counter, 'o' , markersize = 0.4 , color = 'maroon' ) [<matplotlib.lines.Line2D at 0x11fee73d0>]","title":"1. Random Walk in 1D"},{"location":"python/python_numpy_scipy/#hw4","text":"Perform the same 1D random walk using an array with zero values and populating the array at each random walk steps.","title":"HW4"},{"location":"python/python_numpy_scipy/#2-random-walk-in-2d","text":"# Random Walk in 2D steps = 100 counter = [ [ 0 , 0 ] ] for i in range (steps): rx = 2 * np . random . rand() - 1 ry = 2 * np . random . rand() - 1 last_point = counter[ - 1 ] new_point = np . array(last_point) + np . array([rx, ry]) counter . append(new_point) counter = np . array(counter) plt . plot(counter[:, 0 ], counter[:, 1 ], '-o' , lw = 0.5 , color = 'maroon' ) plt . scatter([ 0 ],[ 0 ], marker = 'o' , lw = 10 , color = 'blue' ) plt . scatter(counter[ - 1 ][ 0 ],counter[ - 1 ][ 1 ], marker = '*' , lw = 10 , color = 'green' ) plt . show()","title":"2. Random Walk in 2D"},{"location":"python/python_numpy_scipy/#hw5","text":"Perform the Similar 2D random walk with the counter being initialized as a zero Matrix (HINT: use np.zeros()) and the random number for each step in the range [-2, 2]. HINT: Change the scale of the random number which originally is in the range [0,1) to your range. Compute the average values of x coordinates as well as the y coordinates of all the points visited. HINT: use np.mean() Calculate the distance between first point and the last point visited. # HINT: Use standard mathematical formula Calculate the longest and the shortest distances of all the points (except the origin) from starting point., HINT: use np.max() and np.min()","title":"HW5"},{"location":"python/python_numpy_scipy/#6-calculating-value-of-pi-using-random-numbers","text":"Imagine a circle of radius r inscribed in a square of length 2*r. Through random number on the square. Check if it hits the circle or not. Count the points hitting the circle. For large N (number of dots) the total number of dots represent the area of the square. and the total dots inside the circle represents the are of the circle. Take ratio of area of the two. $$\\frac{A_{circle}}{A_{Square}} = \\frac{ \\pi r^2} { (2r)^2} = \\pi/4$$ $$\\pi = 4 \\frac{A_{circle}}{A_{Square}}$$ $$\\pi = 4 \\frac{N_{circle}}{N_{Square}}$$ # np.random.rand() gives values [0, 1) Maxiter = 100 N_circle = 0 N_square = 0 plt . figure(figsize = ( 6 , 6 )) for i in range (Maxiter): rx = 2 * np . random . rand() - 1 ry = 2 * np . random . rand() - 1 l = np . sqrt(rx ** 2 + ry ** 2 ) plt . scatter(rx, ry, color = 'blue' , lw = '.5' ) if l <= 1. : N_circle = N_circle + 1 plt . scatter(rx, ry, color = 'red' , lw = '.5' ) N_square += 1 plt . xlim([ - 1.2 , 1.2 ]) plt . ylim([ - 1.2 , 1.2 ]) plt . axhline(y = 0. , color = 'maroon' , lw = 2. ) plt . axvline(x = 0. , color = 'maroon' , lw = 2. ) print (N_circle, N_square) pi = 4 * N_circle / N_square circle = np . zeros(( 50 , 2 ), dtype = float ) theta = np . linspace( 0 , 2 * np . pi, 50 ) xt = np . cos(theta) yt = np . sin(theta) plt . plot(xt, yt, color = 'maroon' ) err = ( abs (pi - np . pi) / np . pi) * 100. print ( 'calculated pi=' ,pi, 'Exact pi=' , np . pi, 'Error=' , err, '%' ) 82 100 calculated pi= 3.28 Exact pi= 3.141592653589793 Error= 4.405642668283338 %","title":"6. Calculating value of $\\pi$ using random numbers"},{"location":"python/python_numpy_scipy/#hw6","text":"In the above calculation of $\\pi$, draw a figure with circle inscribed in a square with all of your random dots. Use different colors for the dots inside and outside the circle. Plot vertical and horizontal line passing through the origin for the axis. Hints * You can use plt.axvline and plt.axhline for vertical and horizontal lines. and these can be used to draw the outer lines of the square as well. Just change the location of the lines. * Plotting a circle can be done in many ways, one way is to first generate points on the circle using $(r cos(\\theta), r sin(\\theta) )$ and taking $r=1$ and $\\theta \\in [0, 2\\pi]$ * Plotting dots can be done by doing a scatter plot at each step. Plot the inside dots inside your if (l<=1) statement. * You can use plt.figure(figsize=(8,8)) to make the aspect ratio same for both the axis. * First use only","title":"HW6"},{"location":"python/python_numpy_scipy/#mandelbrot-set-optional","text":"Here is Wikipedia page on Mandelbrot Set. The Mandelbrot set is the set of values of c in the complex plane for which the orbit of 0 under iteration of the quadratic map $$z_{n+1}=z_{n}^{2}+c$$ remains bounded. A point $c$ belongs to the Mandelbrot set if and only if $ \\vert P_{c}^{n}(0) \\vert \\leq 2 $ for all $n \\ge 0 $ import numpy as np import pylab as plt import seaborn as sns #sns.set(False) def CheckBoundedness (c, Maxiter): z = 0 j for i in range (Maxiter): z = z ** ( 2 ) + c if abs (z) > 2 : return i return Maxiter N = 100 ; Maxiter = 80 ; xmin, xmax = [ - 2 , 1 ] ymin, ymax = [ - 2 , 2 ] xL = np . linspace(xmin, xmax, N) yL = np . linspace(ymin, ymax, N) data = np . zeros( (N, N), dtype = float ) for i in range (N): for j in range (N): c = xL[i] + 1 j * yL[j] data[i,j] = CheckBoundedness(c, Maxiter) plt . figure(figsize = ( 6 , 6 )); plt . imshow(data[:: - 1 ] . T, cmap = 'plasma' , extent = [xmin, xmax, ymin, ymax]); #'Spectral' is good #plt.xticks([]); #plt.yticks([]);","title":"Mandelbrot set [Optional]"},{"location":"python/python_numpy_scipy/#simple-pendulum","text":"$$ \\frac{d^2 \\theta}{dt^2} + a \\theta + b \\frac{d \\theta}{dt} = 0 $$ let $y = [ \\theta, \\omega ] $ where $\\omega = \\frac{d\\theta}{dt}$ Hence $$ \\frac{dy}{dt} = \\frac{d}{dt} [\\theta, \\omega] = \\frac{d}{dt} [\\theta, \\frac{d\\theta}{dt}] = [ \\frac{d \\theta}{dt}, \\frac{d^2 \\theta}{dt}] = [ \\frac{d \\theta}{dt} , -a \\theta - b \\frac{d \\theta}{dt} ] = [\\omega, -a \\theta - b \\omega] $$ import numpy as np import pylab as plt def pendulum (y,t, a, b): # y = [\\theta, \\omega] # we return [d \\theta /dt, -a \\theta - b \\omega] dydt = [ y[ 1 ], - a * y[ 0 ] - b * y[ 1 ] ] #dydt= [ y[1], -a*y[0] ] return np . array(dydt) y0 = [ 5. , 0. ] a = 1. b = 0.1 t = np . linspace( 0 , 80 , 500 ) from scipy.integrate import odeint sol = odeint(pendulum, y0, t, args = (a,b)) plt . figure( 1 ) #plt.subplot(121) plt . plot(t, sol[:, 0 ], label = '$ \\\\ theta(t)$' ); plt . plot(t, sol[:, 1 ], label = '$\\omega(t)$' ); plt . legend(frameon = False , fontsize = 16 ); plt . axhline(y = 0 , color = 'k' , ls = ':' );","title":"Simple Pendulum"},{"location":"python/python_pandas/","text":"Data Visualization with Pandas Downloading online data from jupyter Handling the datetime in pandas Data visualzation using covid-19 data subplots using matplotlib curve-fitting and modeling of COVID-19 data. ! ls #!pwd class-wk4.ipynb wk4-class_notes.ipynb ! wget https: // covid . ourworldindata . org / data / ecdc / total_cases . csv ! wget https: // covid . ourworldindata . org / data / ecdc / total_deaths . csv --2020-05-09 10:38:54-- https://covid.ourworldindata.org/data/ecdc/total_cases.csv Resolving covid.ourworldindata.org (covid.ourworldindata.org)... 104.248.50.87 Connecting to covid.ourworldindata.org (covid.ourworldindata.org)|104.248.50.87|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 70071 (68K) [text/csv] Saving to: \u2018total_cases.csv\u2019 total_cases.csv 100%[===================>] 68.43K --.-KB/s in 0.05s 2020-05-09 10:38:54 (1.43 MB/s) - \u2018total_cases.csv\u2019 saved [70071/70071] --2020-05-09 10:38:54-- https://covid.ourworldindata.org/data/ecdc/total_deaths.csv Resolving covid.ourworldindata.org (covid.ourworldindata.org)... 104.248.50.87 Connecting to covid.ourworldindata.org (covid.ourworldindata.org)|104.248.50.87|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 54625 (53K) [text/csv] Saving to: \u2018total_deaths.csv\u2019 total_deaths.csv 100%[===================>] 53.34K --.-KB/s in 0.03s 2020-05-09 10:38:54 (1.57 MB/s) - \u2018total_deaths.csv\u2019 saved [54625/54625] import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd . read_csv( 'total_deaths.csv' ) df . head( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe 0 2019-12-31 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 1 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 3 rows \u00d7 211 columns df . tail( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe 129 2020-05-08 269249 106.0 31.0 483.0 47.0 2.0 0.0 3.0 282.0 ... 4.0 17.0 10.0 0.0 10.0 0.0 0.0 6.0 4.0 4.0 130 2020-05-09 274290 109.0 31.0 488.0 47.0 2.0 0.0 3.0 293.0 ... 4.0 18.0 10.0 0.0 10.0 0.0 0.0 7.0 4.0 4.0 2 rows \u00d7 211 columns df . columns Index(['date', 'World', 'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Anguilla', 'Antigua and Barbuda', 'Argentina', ... 'United States Virgin Islands', 'Uruguay', 'Uzbekistan', 'Vatican', 'Venezuela', 'Vietnam', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe'], dtype='object', length=211) df . dtypes date object World int64 Afghanistan float64 Albania float64 Algeria float64 ... Vietnam float64 Western Sahara float64 Yemen float64 Zambia float64 Zimbabwe float64 Length: 211, dtype: object df = pd . read_csv( 'total_deaths.csv' , parse_dates = [ 'date' ], index_col = [ 'date' ] ) df . head( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 3 rows \u00d7 210 columns df . index DatetimeIndex(['2019-12-31', '2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08', '2020-01-09', ... '2020-04-30', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09'], dtype='datetime64[ns]', name='date', length=131, freq=None) # data from fixed date period df[ '2020-01-01' : '2020-01-11' ] df_jan .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-03 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-04 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-05 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-06 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-07 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-08 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-09 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-10 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-11 1 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 11 rows \u00d7 210 columns df . index DatetimeIndex(['2019-12-31', '2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08', '2020-01-09', ... '2020-04-30', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09'], dtype='datetime64[ns]', name='date', length=131, freq=None) df . index . strftime( '%m- %d ' ) Index(['12-31', '01-01', '01-02', '01-03', '01-04', '01-05', '01-06', '01-07', '01-08', '01-09', ... '04-30', '05-01', '05-02', '05-03', '05-04', '05-05', '05-06', '05-07', '05-08', '05-09'], dtype='object', length=131) Data preprocessing dfd = pd . read_csv( 'total_deaths.csv' , parse_dates = [ 'date' ], index_col = [ 'date' ] ) dfc = pd . read_csv( 'total_cases.csv' , parse_dates = [ 'date' ], index_col = [ 'date' ] ) dfd . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-03 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-04 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 5 rows \u00d7 210 columns dfc . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 27 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-01 27 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 27 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-03 44 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-04 44 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 5 rows \u00d7 210 columns # check for NaN in the entire dataframe dfd . isnull() . values . any() True # count the NaN under each columns dfd . isnull() . sum() World 0 Afghanistan 10 Albania 69 Algeria 5 Andorra 74 ... Vietnam 4 Western Sahara 117 Yemen 101 Zambia 79 Zimbabwe 81 Length: 210, dtype: int64 # Count total NaN in the dataframe dfd . isnull() . sum() . sum() 11473 # Replace the NaN by some numbers dfd = dfd . fillna( 0 ) # dfd.fillna( dfd.mean() ) dfc = dfc . fillna( 0 ) dfd . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-01 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 rows \u00d7 210 columns dfc . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-01 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-04 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows \u00d7 210 columns Data visualization # taking out data of a column # two methods dfd . Afghanistan date 2019-12-31 0.0 2020-01-01 0.0 2020-01-02 0.0 2020-01-03 0.0 2020-01-04 0.0 ... 2020-05-05 90.0 2020-05-06 95.0 2020-05-07 104.0 2020-05-08 106.0 2020-05-09 109.0 Name: Afghanistan, Length: 131, dtype: float64 #dfd.United States # doesn't work #method 2 dfd[ 'United States' ] date 2019-12-31 0 2020-01-01 0 2020-01-02 0 2020-01-03 0 2020-01-04 0 ... 2020-05-05 68934 2020-05-06 71078 2020-05-07 73431 2020-05-08 75670 2020-05-09 77180 Name: United States, Length: 131, dtype: int64 dfd . columns Index(['World', 'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Anguilla', 'Antigua and Barbuda', 'Argentina', 'Armenia', ... 'United States Virgin Islands', 'Uruguay', 'Uzbekistan', 'Vatican', 'Venezuela', 'Vietnam', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe'], dtype='object', length=210) # plot Chinese data % matplotlib inline plt . plot(dfd . index, dfd . China) [<matplotlib.lines.Line2D at 0x11525d490>] plt . plot(dfd . index, dfd . India) [<matplotlib.lines.Line2D at 0x112e05cd0>] plt . plot(dfd . index, dfd[ 'United States' ]) [<matplotlib.lines.Line2D at 0x115393fd0>] import seaborn as sns sns . set_style( 'whitegrid' ) plt . plot(dfd . index, dfd[ 'United States' ]) [<matplotlib.lines.Line2D at 0x11d14f110>] plt . plot(dfd . index, dfd[ 'United States' ]) plt . xticks(rotation = 45 ); (array([737425., 737446., 737456., 737477., 737485., 737506., 737516., 737537., 737546.]), <a list of 9 Text xticklabel objects>) plt . plot(dfd . index, dfc[ 'Nepal' ]) plt . xticks(rotation = 45 ); plt . figure(figsize = ( 15 , 5 )) plt . subplot( 1 , 3 , 1 ) plt . plot(dfd . index, dfc[ 'Nepal' ]) plt . title( 'Nepal' ) plt . xticks(rotation = 45 ); plt . subplot( 1 , 3 , 2 ) plt . plot(dfd . index, dfc[ 'India' ]) plt . title( 'India' ) plt . xticks(rotation = 45 ); plt . subplot( 1 , 3 , 3 ) plt . plot(dfd . index, dfc[ 'Pakistan' ]) plt . title( 'Pakistan' ) plt . xticks(rotation = 45 ); countries = [ 'Nepal' , 'India' , 'Pakistan' ] #for i in range(3): # print (i, countries[i]) #using enumerate plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , 3 , 1 + i) plt . plot(dfd . index, dfd[c], label = 'Deaths' ) plt . plot(dfc . index, dfc[c], label = 'Cases' ) plt . legend(fontsize = 12 , frameon = False ) plt . title(c, fontsize = 16 , color = 'maroon' ) plt . xticks(rotation = 45 ); #print (i, c) countries = [ 'United States' , 'Italy' , 'France' , 'United Kingdom' , 'China' , 'Germany' ] plt . figure(figsize = ( 15 , 10 )) for i, c in enumerate (countries): plt . subplot( 2 , len (countries) // 2 , 1 + i) #plt.plot(dfd.index, dfd[c], label='Deaths') #plt.plot(dfc.index, dfc[c], label='Cases') plt . plot(dfd . index, dfd[c], 'o' , label = 'Deaths' ) plt . plot(dfc . index, dfc[c], 'o' , label = 'Cases' ) plt . legend(fontsize = 12 , frameon = False ) plt . title(c, fontsize = 16 , color = 'maroon' ) plt . xticks(rotation = 45 ); #1000 cases in USA N = 1000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) N = 10000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) N = 100000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) N = 500000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) 2020-03-11 00:00:00 2020-03-20 00:00:00 2020-03-28 00:00:00 2020-04-11 00:00:00 milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] for i, n in enumerate (milestones): print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] ) 2020-03-11 00:00:00 2020-03-20 00:00:00 2020-03-28 00:00:00 2020-04-11 00:00:00 2020-04-29 00:00:00 milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] for i, n in enumerate (milestones): indx = dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] plt . scatter( n, indx) #plt.axvline(x=n) #plt.xticks(milestones, milestones) milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] countries = [ 'United States' , 'Italy' , 'France' ] #, 'United Kingdom', 'China', 'Germany'] plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , len (countries), i + 1 ) plt . title(c) for j, n in enumerate (milestones): indx = dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] plt . scatter( n, indx, color = 'blue' ) #plt.axvline(x=n) #plt.xticks(milestones, milestones) milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] countries = [ 'United States' , 'Italy' , 'France' ] #, 'United Kingdom', 'China', 'Germany'] plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , len (countries), i + 1 ) plt . title(c) for j, n in enumerate (milestones): indx = dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] plt . scatter( n, indx . strftime( '%m- %d ' ), color = 'blue' ) #plt.axvline(x=n) #plt.xticks(milestones, milestones) Curve fitting def func (x, a, b, c): return a * np . exp( - b * x) + c xdata = np . linspace( 0 , 10 , 100 ) y = func(xdata, 2.5 , 1.3 , 0.5 ) #plt.plot (xdata, y, 'o') y_noise = 0.5 * np . random . rand( 100 ) ydata = y + y_noise plt . plot (xdata, ydata, 'o' ) from scipy.optimize import curve_fit popt, pcov = curve_fit(func, xdata, ydata) print (popt) plt . plot(xdata, func(xdata, popt[ 0 ], popt[ 1 ], popt[ 2 ]), color = 'red' ) [2.4696733 1.31979388 0.75564748] [<matplotlib.lines.Line2D at 0x11d580510>] def sigmoid (x, L, x0, k): return L / ( 1 + np . exp( - k * (x - x0))) xx = np . array( range ( len (dfc . index)) ) yy = dfc . Italy plt . plot(xx, yy, 'o' , label = 'Cases: data' ) mean = np . mean(yy) sig = np . std(yy) p0 = [ 1 , mean, 1. ] popt, pcov = curve_fit(sigmoid, xx, yy) #, p0) #print (popt) #plt.plot(xx, sigmoid(xx, popt[0], popt[1], popt[2]), color='red') plt . plot(xx, sigmoid(xx, * popt), label = 'Fit' ) plt . legend(); # for prediction plt . plot(xx, yy, 'o' , label = 'Cases: data' ) xnew = np . array( range ( 180 ) ) plt . plot(xnew, sigmoid(xnew, * popt), label = 'Fit' ) plt . legend(); Fitting the daily cases/deaths dfd1 = dfd . diff() dfc1 = dfc . diff() dfd1 . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-01 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-04 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows \u00d7 210 columns dfd1 = dfd . diff() . fillna( 0 ) dfc1 = dfc . diff() . fillna( 0 ) #dfd1.head() dfd1 . tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2020-05-05 3999.0 5.0 0.0 2.0 0.0 0.0 0.0 0.0 14.0 4.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-05-06 5952.0 5.0 0.0 5.0 1.0 0.0 0.0 0.0 4.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 2020-05-07 6387.0 9.0 0.0 6.0 0.0 0.0 0.0 0.0 9.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 2020-05-08 5748.0 2.0 0.0 7.0 1.0 0.0 0.0 0.0 9.0 2.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 2020-05-09 5041.0 3.0 0.0 5.0 0.0 0.0 0.0 0.0 11.0 1.0 ... 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 5 rows \u00d7 210 columns plt . plot(dfd1 . index, dfd1[ 'United Kingdom' ], 'o' ) [<matplotlib.lines.Line2D at 0x11dd61110>] def Gaussian (x, x0, sigma, a): return a * np . exp( - (x - x0) ** 2 / ( 2 * sigma ** 2 ) ) xx = np . array( range ( len (dfc . index)) ) yy = dfd1[ 'United Kingdom' ] plt . scatter(xx, yy) p0 = [ np . median(yy), np . std(yy), np . max(yy) ] popt, pcov = curve_fit(Gaussian, xx, yy, p0) plt . plot(xx, Gaussian(xx, * popt), color = 'red' ) plt . title( 'United Kingdom' ) Text(0.5, 1.0, 'United Kingdom') countries = [ 'United States' , 'Italy' , 'France' ] #, 'United Kingdom', 'China', 'Germany'] plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , len (countries), i + 1 ) plt . title(c) xx = np . array( range ( len (dfc . index)) ) yy = dfd1[c] plt . scatter(xx, yy) p0 = [ np . median(yy), np . std(yy), np . max(yy) ] popt, pcov = curve_fit(Gaussian, xx, yy, p0) plt . plot(xx, Gaussian(xx, * popt), color = 'red' )","title":"Pandas examples"},{"location":"python/python_pandas/#data-visualization-with-pandas","text":"Downloading online data from jupyter Handling the datetime in pandas Data visualzation using covid-19 data subplots using matplotlib curve-fitting and modeling of COVID-19 data. ! ls #!pwd class-wk4.ipynb wk4-class_notes.ipynb ! wget https: // covid . ourworldindata . org / data / ecdc / total_cases . csv ! wget https: // covid . ourworldindata . org / data / ecdc / total_deaths . csv --2020-05-09 10:38:54-- https://covid.ourworldindata.org/data/ecdc/total_cases.csv Resolving covid.ourworldindata.org (covid.ourworldindata.org)... 104.248.50.87 Connecting to covid.ourworldindata.org (covid.ourworldindata.org)|104.248.50.87|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 70071 (68K) [text/csv] Saving to: \u2018total_cases.csv\u2019 total_cases.csv 100%[===================>] 68.43K --.-KB/s in 0.05s 2020-05-09 10:38:54 (1.43 MB/s) - \u2018total_cases.csv\u2019 saved [70071/70071] --2020-05-09 10:38:54-- https://covid.ourworldindata.org/data/ecdc/total_deaths.csv Resolving covid.ourworldindata.org (covid.ourworldindata.org)... 104.248.50.87 Connecting to covid.ourworldindata.org (covid.ourworldindata.org)|104.248.50.87|:443... connected. HTTP request sent, awaiting response... 200 OK Length: 54625 (53K) [text/csv] Saving to: \u2018total_deaths.csv\u2019 total_deaths.csv 100%[===================>] 53.34K --.-KB/s in 0.03s 2020-05-09 10:38:54 (1.57 MB/s) - \u2018total_deaths.csv\u2019 saved [54625/54625] import pandas as pd import numpy as np import matplotlib.pyplot as plt df = pd . read_csv( 'total_deaths.csv' ) df . head( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe 0 2019-12-31 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 1 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 3 rows \u00d7 211 columns df . tail( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } date World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe 129 2020-05-08 269249 106.0 31.0 483.0 47.0 2.0 0.0 3.0 282.0 ... 4.0 17.0 10.0 0.0 10.0 0.0 0.0 6.0 4.0 4.0 130 2020-05-09 274290 109.0 31.0 488.0 47.0 2.0 0.0 3.0 293.0 ... 4.0 18.0 10.0 0.0 10.0 0.0 0.0 7.0 4.0 4.0 2 rows \u00d7 211 columns df . columns Index(['date', 'World', 'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Anguilla', 'Antigua and Barbuda', 'Argentina', ... 'United States Virgin Islands', 'Uruguay', 'Uzbekistan', 'Vatican', 'Venezuela', 'Vietnam', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe'], dtype='object', length=211) df . dtypes date object World int64 Afghanistan float64 Albania float64 Algeria float64 ... Vietnam float64 Western Sahara float64 Yemen float64 Zambia float64 Zimbabwe float64 Length: 211, dtype: object df = pd . read_csv( 'total_deaths.csv' , parse_dates = [ 'date' ], index_col = [ 'date' ] ) df . head( 3 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 3 rows \u00d7 210 columns df . index DatetimeIndex(['2019-12-31', '2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08', '2020-01-09', ... '2020-04-30', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09'], dtype='datetime64[ns]', name='date', length=131, freq=None) # data from fixed date period df[ '2020-01-01' : '2020-01-11' ] df_jan .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-03 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-04 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-05 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-06 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-07 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-08 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-09 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-10 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-11 1 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 11 rows \u00d7 210 columns df . index DatetimeIndex(['2019-12-31', '2020-01-01', '2020-01-02', '2020-01-03', '2020-01-04', '2020-01-05', '2020-01-06', '2020-01-07', '2020-01-08', '2020-01-09', ... '2020-04-30', '2020-05-01', '2020-05-02', '2020-05-03', '2020-05-04', '2020-05-05', '2020-05-06', '2020-05-07', '2020-05-08', '2020-05-09'], dtype='datetime64[ns]', name='date', length=131, freq=None) df . index . strftime( '%m- %d ' ) Index(['12-31', '01-01', '01-02', '01-03', '01-04', '01-05', '01-06', '01-07', '01-08', '01-09', ... '04-30', '05-01', '05-02', '05-03', '05-04', '05-05', '05-06', '05-07', '05-08', '05-09'], dtype='object', length=131)","title":"Data Visualization with Pandas"},{"location":"python/python_pandas/#data-preprocessing","text":"dfd = pd . read_csv( 'total_deaths.csv' , parse_dates = [ 'date' ], index_col = [ 'date' ] ) dfc = pd . read_csv( 'total_cases.csv' , parse_dates = [ 'date' ], index_col = [ 'date' ] ) dfd . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-01 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-03 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-04 0 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 5 rows \u00d7 210 columns dfc . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 27 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-01 27 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-02 27 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-03 44 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 2020-01-04 44 0.0 NaN 0.0 NaN NaN NaN NaN NaN 0.0 ... NaN NaN NaN NaN NaN 0.0 NaN NaN NaN NaN 5 rows \u00d7 210 columns # check for NaN in the entire dataframe dfd . isnull() . values . any() True # count the NaN under each columns dfd . isnull() . sum() World 0 Afghanistan 10 Albania 69 Algeria 5 Andorra 74 ... Vietnam 4 Western Sahara 117 Yemen 101 Zambia 79 Zimbabwe 81 Length: 210, dtype: int64 # Count total NaN in the dataframe dfd . isnull() . sum() . sum() 11473 # Replace the NaN by some numbers dfd = dfd . fillna( 0 ) # dfd.fillna( dfd.mean() ) dfc = dfc . fillna( 0 ) dfd . head( 2 ) .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-01 0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2 rows \u00d7 210 columns dfc . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-01 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 27 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-04 44 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows \u00d7 210 columns","title":"Data preprocessing"},{"location":"python/python_pandas/#data-visualization","text":"# taking out data of a column # two methods dfd . Afghanistan date 2019-12-31 0.0 2020-01-01 0.0 2020-01-02 0.0 2020-01-03 0.0 2020-01-04 0.0 ... 2020-05-05 90.0 2020-05-06 95.0 2020-05-07 104.0 2020-05-08 106.0 2020-05-09 109.0 Name: Afghanistan, Length: 131, dtype: float64 #dfd.United States # doesn't work #method 2 dfd[ 'United States' ] date 2019-12-31 0 2020-01-01 0 2020-01-02 0 2020-01-03 0 2020-01-04 0 ... 2020-05-05 68934 2020-05-06 71078 2020-05-07 73431 2020-05-08 75670 2020-05-09 77180 Name: United States, Length: 131, dtype: int64 dfd . columns Index(['World', 'Afghanistan', 'Albania', 'Algeria', 'Andorra', 'Angola', 'Anguilla', 'Antigua and Barbuda', 'Argentina', 'Armenia', ... 'United States Virgin Islands', 'Uruguay', 'Uzbekistan', 'Vatican', 'Venezuela', 'Vietnam', 'Western Sahara', 'Yemen', 'Zambia', 'Zimbabwe'], dtype='object', length=210) # plot Chinese data % matplotlib inline plt . plot(dfd . index, dfd . China) [<matplotlib.lines.Line2D at 0x11525d490>] plt . plot(dfd . index, dfd . India) [<matplotlib.lines.Line2D at 0x112e05cd0>] plt . plot(dfd . index, dfd[ 'United States' ]) [<matplotlib.lines.Line2D at 0x115393fd0>] import seaborn as sns sns . set_style( 'whitegrid' ) plt . plot(dfd . index, dfd[ 'United States' ]) [<matplotlib.lines.Line2D at 0x11d14f110>] plt . plot(dfd . index, dfd[ 'United States' ]) plt . xticks(rotation = 45 ); (array([737425., 737446., 737456., 737477., 737485., 737506., 737516., 737537., 737546.]), <a list of 9 Text xticklabel objects>) plt . plot(dfd . index, dfc[ 'Nepal' ]) plt . xticks(rotation = 45 ); plt . figure(figsize = ( 15 , 5 )) plt . subplot( 1 , 3 , 1 ) plt . plot(dfd . index, dfc[ 'Nepal' ]) plt . title( 'Nepal' ) plt . xticks(rotation = 45 ); plt . subplot( 1 , 3 , 2 ) plt . plot(dfd . index, dfc[ 'India' ]) plt . title( 'India' ) plt . xticks(rotation = 45 ); plt . subplot( 1 , 3 , 3 ) plt . plot(dfd . index, dfc[ 'Pakistan' ]) plt . title( 'Pakistan' ) plt . xticks(rotation = 45 ); countries = [ 'Nepal' , 'India' , 'Pakistan' ] #for i in range(3): # print (i, countries[i]) #using enumerate plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , 3 , 1 + i) plt . plot(dfd . index, dfd[c], label = 'Deaths' ) plt . plot(dfc . index, dfc[c], label = 'Cases' ) plt . legend(fontsize = 12 , frameon = False ) plt . title(c, fontsize = 16 , color = 'maroon' ) plt . xticks(rotation = 45 ); #print (i, c) countries = [ 'United States' , 'Italy' , 'France' , 'United Kingdom' , 'China' , 'Germany' ] plt . figure(figsize = ( 15 , 10 )) for i, c in enumerate (countries): plt . subplot( 2 , len (countries) // 2 , 1 + i) #plt.plot(dfd.index, dfd[c], label='Deaths') #plt.plot(dfc.index, dfc[c], label='Cases') plt . plot(dfd . index, dfd[c], 'o' , label = 'Deaths' ) plt . plot(dfc . index, dfc[c], 'o' , label = 'Cases' ) plt . legend(fontsize = 12 , frameon = False ) plt . title(c, fontsize = 16 , color = 'maroon' ) plt . xticks(rotation = 45 ); #1000 cases in USA N = 1000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) N = 10000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) N = 100000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) N = 500000 print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= N][ 0 ] ) 2020-03-11 00:00:00 2020-03-20 00:00:00 2020-03-28 00:00:00 2020-04-11 00:00:00 milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] for i, n in enumerate (milestones): print ( dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] ) 2020-03-11 00:00:00 2020-03-20 00:00:00 2020-03-28 00:00:00 2020-04-11 00:00:00 2020-04-29 00:00:00 milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] for i, n in enumerate (milestones): indx = dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] plt . scatter( n, indx) #plt.axvline(x=n) #plt.xticks(milestones, milestones) milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] countries = [ 'United States' , 'Italy' , 'France' ] #, 'United Kingdom', 'China', 'Germany'] plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , len (countries), i + 1 ) plt . title(c) for j, n in enumerate (milestones): indx = dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] plt . scatter( n, indx, color = 'blue' ) #plt.axvline(x=n) #plt.xticks(milestones, milestones) milestones = [ 1000 , 10000 , 100000 , 500000 , 1000000 ] countries = [ 'United States' , 'Italy' , 'France' ] #, 'United Kingdom', 'China', 'Germany'] plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , len (countries), i + 1 ) plt . title(c) for j, n in enumerate (milestones): indx = dfc[ 'United States' ] . index[ dfc[ 'United States' ] >= n][ 0 ] plt . scatter( n, indx . strftime( '%m- %d ' ), color = 'blue' ) #plt.axvline(x=n) #plt.xticks(milestones, milestones)","title":"Data visualization"},{"location":"python/python_pandas/#curve-fitting","text":"def func (x, a, b, c): return a * np . exp( - b * x) + c xdata = np . linspace( 0 , 10 , 100 ) y = func(xdata, 2.5 , 1.3 , 0.5 ) #plt.plot (xdata, y, 'o') y_noise = 0.5 * np . random . rand( 100 ) ydata = y + y_noise plt . plot (xdata, ydata, 'o' ) from scipy.optimize import curve_fit popt, pcov = curve_fit(func, xdata, ydata) print (popt) plt . plot(xdata, func(xdata, popt[ 0 ], popt[ 1 ], popt[ 2 ]), color = 'red' ) [2.4696733 1.31979388 0.75564748] [<matplotlib.lines.Line2D at 0x11d580510>] def sigmoid (x, L, x0, k): return L / ( 1 + np . exp( - k * (x - x0))) xx = np . array( range ( len (dfc . index)) ) yy = dfc . Italy plt . plot(xx, yy, 'o' , label = 'Cases: data' ) mean = np . mean(yy) sig = np . std(yy) p0 = [ 1 , mean, 1. ] popt, pcov = curve_fit(sigmoid, xx, yy) #, p0) #print (popt) #plt.plot(xx, sigmoid(xx, popt[0], popt[1], popt[2]), color='red') plt . plot(xx, sigmoid(xx, * popt), label = 'Fit' ) plt . legend(); # for prediction plt . plot(xx, yy, 'o' , label = 'Cases: data' ) xnew = np . array( range ( 180 ) ) plt . plot(xnew, sigmoid(xnew, * popt), label = 'Fit' ) plt . legend();","title":"Curve fitting"},{"location":"python/python_pandas/#fitting-the-daily-casesdeaths","text":"dfd1 = dfd . diff() dfc1 = dfc . diff() dfd1 . head() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2019-12-31 NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN ... NaN NaN NaN NaN NaN NaN NaN NaN NaN NaN 2020-01-01 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-02 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-03 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-01-04 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 5 rows \u00d7 210 columns dfd1 = dfd . diff() . fillna( 0 ) dfc1 = dfc . diff() . fillna( 0 ) #dfd1.head() dfd1 . tail() .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; } World Afghanistan Albania Algeria Andorra Angola Anguilla Antigua and Barbuda Argentina Armenia ... United States Virgin Islands Uruguay Uzbekistan Vatican Venezuela Vietnam Western Sahara Yemen Zambia Zimbabwe date 2020-05-05 3999.0 5.0 0.0 2.0 0.0 0.0 0.0 0.0 14.0 4.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2020-05-06 5952.0 5.0 0.0 5.0 1.0 0.0 0.0 0.0 4.0 1.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 1.0 0.0 2020-05-07 6387.0 9.0 0.0 6.0 0.0 0.0 0.0 0.0 9.0 0.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 2.0 0.0 0.0 2020-05-08 5748.0 2.0 0.0 7.0 1.0 0.0 0.0 0.0 9.0 2.0 ... 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 2020-05-09 5041.0 3.0 0.0 5.0 0.0 0.0 0.0 0.0 11.0 1.0 ... 0.0 1.0 0.0 0.0 0.0 0.0 0.0 1.0 0.0 0.0 5 rows \u00d7 210 columns plt . plot(dfd1 . index, dfd1[ 'United Kingdom' ], 'o' ) [<matplotlib.lines.Line2D at 0x11dd61110>] def Gaussian (x, x0, sigma, a): return a * np . exp( - (x - x0) ** 2 / ( 2 * sigma ** 2 ) ) xx = np . array( range ( len (dfc . index)) ) yy = dfd1[ 'United Kingdom' ] plt . scatter(xx, yy) p0 = [ np . median(yy), np . std(yy), np . max(yy) ] popt, pcov = curve_fit(Gaussian, xx, yy, p0) plt . plot(xx, Gaussian(xx, * popt), color = 'red' ) plt . title( 'United Kingdom' ) Text(0.5, 1.0, 'United Kingdom') countries = [ 'United States' , 'Italy' , 'France' ] #, 'United Kingdom', 'China', 'Germany'] plt . figure(figsize = ( 15 , 5 )) for i, c in enumerate (countries): plt . subplot( 1 , len (countries), i + 1 ) plt . title(c) xx = np . array( range ( len (dfc . index)) ) yy = dfd1[c] plt . scatter(xx, yy) p0 = [ np . median(yy), np . std(yy), np . max(yy) ] popt, pcov = curve_fit(Gaussian, xx, yy, p0) plt . plot(xx, Gaussian(xx, * popt), color = 'red' )","title":"Fitting the daily cases/deaths"}]}